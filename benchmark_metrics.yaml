summary:
  mmdetection:
    total_configs: 100
    total_models: 821
  mmpose:
    total_configs: 18
    total_models: 215
papers:
- title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
  algorithm: Faster R-CNN
  abstract: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection
    networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus
    enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to
    generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently
    popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of
    5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015
    competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks.
- title: Mask R-CNN
  algorithm: Mask R-CNN
  abstract: We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality
    segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition.
    Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in
    the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells
    and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline
    and help ease future research in instance-level recognition.
- title: Bottom-up Human Pose Estimation via Disentangled Keypoint Regression
  algorithm: DEKR
  abstract: In this paper, we study the problem of bottom-up human pose estimation from multi-person images. Existing approaches regress all keypoints of instances indiscriminately, ignoring the fact that
    keypoints belonging to different persons have varying difficulties in prediction. To solve this problem, we propose to use disentangled keypoint regression, which adopts adaptive convolutions and multi-scale
    feature fusion to produce accurate predictions. The proposed method predicts all the keypoints in multi-person scenarios simultaneously and efficiently. Moreover, we develop a novel center-to-joint
    offset branch that can further boost the performance. Extensive experiments on the COCO dataset demonstrate that our method outperforms all existing bottom-up methods.
- title: 'Libra R-CNN: Towards Balanced Learning for Object Detection'
  algorithm: Libra R-CNN
  abstract: 'Compared with model architectures, the training process, which is also crucial to the success of detectors, has received relatively less attention in object detection. In this work, we carefully
    revisit the standard training practice of detectors, and find that the detection performance is often limited by the imbalance during the training process, which generally consists in three levels -
    sample level, feature level, and objective level. To mitigate the adverse effects caused thereby, we propose Libra R-CNN, a simple but effective framework towards balanced learning for object detection.
    It integrates three novel components: IoU-balanced sampling, balanced feature pyramid, and balanced L1 loss, respectively for reducing the imbalance at sample, feature, and objective level. Benefitted
    from the overall balanced design, Libra R-CNN significantly improves the detection performance. Without bells and whistles, it achieves 2.5 points and 2.0 points higher Average Precision (AP) than FPN
    Faster R-CNN and RetinaNet respectively on MSCOCO.'
- title: 'SSD: Single Shot MultiBox Detector'
  algorithm: SSD
  abstract: We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over
    different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to
    the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model
    is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in
    a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets
    confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared
    to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For 300×300 input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for 500×500
    input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model.
- title: Hybrid Task Cascade for Instance Segmentation
  algorithm: HTC
  abstract: 'Cascade is a classic yet powerful architecture that has boosted performance on various tasks. However, how to introduce cascade to instance segmentation remains an open question. A simple combination
    of Cascade R-CNN and Mask R-CNN only brings limited gain. In exploring a more effective approach, we find that the key to a successful instance segmentation cascade is to fully leverage the reciprocal
    relationship between detection and segmentation. In this work, we propose a new framework, Hybrid Task Cascade (HTC), which differs in two important aspects: (1) instead of performing cascaded refinement
    on these two tasks separately, it interweaves them for a joint multi-stage processing; (2) it adopts a fully convolutional branch to provide spatial context, which can help distinguishing hard foreground
    from cluttered background. Overall, this framework can learn more discriminative features progressively while integrating complementary features together in each stage. Without bells and whistles, a
    single HTC obtains 38.4 and 1.5 improvement over a strong Cascade Mask R-CNN baseline on MSCOCO dataset. Moreover, our overall system achieves 48.6 mask AP on the test-challenge split, ranking 1st in
    the COCO 2018 Challenge Object Detection Task.'
- title: A novel Region of Interest Extraction Layer for Instance Segmentation
  algorithm: GRoIE
  abstract: Given the wide diffusion of deep neural network architectures for computer vision tasks, several new applications are nowadays more and more feasible. Among them, a particular attention has
    been recently given to instance segmentation, by exploiting the results achievable by two-stage networks (such as Mask R-CNN or Faster R-CNN), derived from R-CNN. In these complex architectures, a crucial
    role is played by the Region of Interest (RoI) extraction layer, devoted to extracting a coherent subset of features from a single Feature Pyramid Network (FPN) layer attached on top of a backbone.
    This paper is motivated by the need to overcome the limitations of existing RoI extractors which select only one (the best) layer from FPN. Our intuition is that all the layers of FPN retain useful
    information. Therefore, the proposed layer (called Generic RoI Extractor - GRoIE) introduces non-local building blocks and attention mechanisms to boost the performance. A comprehensive ablation study
    at component level is conducted to find the best set of algorithms and parameters for the GRoIE layer. Moreover, GRoIE can be integrated seamlessly with every two-stage architecture for both object
    detection and instance segmentation tasks. Therefore, the improvements brought about by the use of GRoIE in different state-of-the-art architectures are also evaluated. The proposed layer leads up to
    gain a 1.1% AP improvement on bounding box detection and 1.7% AP improvement on instance segmentation.
- title: Region Proposal by Guided Anchoring
  algorithm: Guided Anchoring
  abstract: Region anchors are the cornerstone of modern object detection techniques. State-of-the-art detectors mostly rely on a dense anchoring scheme, where anchors are sampled uniformly over the spatial
    domain with a predefined set of scales and aspect ratios. In this paper, we revisit this foundational stage. Our study shows that it can be done much more effectively and efficiently. Specifically,
    we present an alternative scheme, named Guided Anchoring, which leverages semantic features to guide the anchoring. The proposed method jointly predicts the locations where the center of objects of
    interest are likely to exist as well as the scales and aspect ratios at different locations. On top of predicted anchor shapes, we mitigate the feature inconsistency with a feature adaption module.
    We also study the use of high-quality proposals to improve detection performance. The anchoring scheme can be seamlessly integrated into proposal methods and detectors. With Guided Anchoring, we achieve
    9.1% higher recall on MS COCO with 90% fewer anchors than the RPN baseline. We also adopt Guided Anchoring in Fast R-CNN, Faster R-CNN and RetinaNet, respectively improving the detection mAP by 2.2%,
    2.7% and 1.2%.
- title: Objects as Points
  algorithm: CenterNet
  abstract: Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful,
    inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint
    estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable,
    simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at
    52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs
    competitively with sophisticated multi-stage methods and runs in real-time.
- title: 'YOLOX: Exceeding YOLO Series in 2021'
  algorithm: YOLOX
  abstract: 'In this report, we present some experienced improvements to YOLO series, forming a new high-performance detector -- YOLOX. We switch the YOLO detector to an anchor-free manner and conduct other
    advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only
    0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming
    the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L
    by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model. We hope this report can provide useful experience
    for developers and researchers in practical scenes, and we also provide deploy versions with ONNX, TensorRT, NCNN, and Openvino supported.'
- title: A ConvNet for the 2020s
  algorithm: ConvNeXt
  abstract: The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla
    ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers)
    that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the
    effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine
    the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that
    contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts
    compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while
    maintaining the simplicity and efficiency of standard ConvNets.
- title: Scale-Aware Trident Networks for Object Detection
  algorithm: TridentNet
  abstract: Scale variation is one of the key challenges in object detection. In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in
    object detection. Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational
    power. We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields. Then, we adopt a scale-aware training scheme
    to specialize each branch by sampling object instances of proper scales for training. As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional
    parameters and computational cost compared with the vanilla detector. On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP.
- title: 'Cascade R-CNN: High Quality Object Detection and Instance Segmentation'
  algorithm: Cascade R-CNN
  abstract: 'In object detection, the intersection over union (IoU) threshold is frequently used to define positives/negatives. The threshold used to train a detector defines its quality. While the commonly
    used threshold of 0.5 leads to noisy (low-quality) detections, detection performance frequently degrades for larger thresholds. This paradox of high-quality detection has two causes: 1) overfitting,
    due to vanishing positive samples for large thresholds, and 2) inference-time quality mismatch between detector and test hypotheses. A multi-stage object detection architecture, the Cascade R-CNN, composed
    of a sequence of detectors trained with increasing IoU thresholds, is proposed to address these problems. The detectors are trained sequentially, using the output of a detector as training set for the
    next. This resampling progressively improves hypotheses quality, guaranteeing a positive training set of equivalent size for all detectors and minimizing overfitting. The same cascade is applied at
    inference, to eliminate quality mismatches between hypotheses and detectors. An implementation of the Cascade R-CNN without bells or whistles achieves state-of-the-art performance on the COCO dataset,
    and significantly improves high-quality detection on generic and specific object detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally, the Cascade R-CNN is generalized to instance
    segmentation, with nontrivial improvements over the Mask R-CNN.'
- algorithm: LVIS
  title: 'LVIS: A Dataset for Large Vocabulary Instance Segmentation'
  abstract: 'Progress on object detection is enabled by datasets that focus the research community''s attention on open challenges. This process led us from simple images to complex scenes and from bounding
    boxes to segmentation masks. In this work, we introduce LVIS (pronounced \`el-vis''): a new dataset for Large Vocabulary Instance Segmentation. We plan to collect ~2 million high-quality instance segmentation
    masks for over 1000 entry-level object categories in 164k images. Due to the Zipfian distribution of categories in natural images, LVIS naturally has a long tail of categories with few training samples.
    Given that state-of-the-art deep learning methods for object detection perform poorly in the low-sample regime, we believe that our dataset poses an important and exciting new scientific challenge.'
- algorithm: CrowdDet
  title: 'Detection in Crowded Scenes: One Proposal, Multiple Predictions'
  abstract: We propose a simple yet effective proposal-based object detector, aiming at detecting highly-overlapped instances in crowded scenes. The key of our approach is to let each proposal predict a
    set of correlated instances rather than a single one in previous proposal-based frameworks. Equipped with new techniques such as EMD Loss and Set NMS, our detector can effectively handle the difficulty
    of detecting highly overlapped objects. On a FPN-Res50 baseline, our detector can obtain 4.9% AP gains on challenging CrowdHuman dataset and 1.0% MR^−2 improvements on CityPersons dataset, without bells
    and whistles. Moreover, on less crowed datasets like COCO, our approach can still achieve moderate improvement, suggesting the proposed method is robust to crowdedness. Code and pre-trained models will
    be released at https://github.com/megvii-model/CrowdDetection.
- algorithm: MS R-CNN
  title: Mask Scoring R-CNN
  abstract: Letting a deep network be aware of the quality of its own predictions is an interesting yet important problem. In the task of instance segmentation, the confidence of instance classification
    is used as mask quality score in most instance segmentation frameworks. However, the mask quality, quantified as the IoU between the instance mask and its ground truth, is usually not well correlated
    with classification score. In this paper, we study this problem and propose Mask Scoring R-CNN which contains a network block to learn the quality of the predicted instance masks. The proposed network
    block takes the instance feature and the corresponding predicted mask together to regress the mask IoU. The mask scoring strategy calibrates the misalignment between mask quality and mask score, and
    improves instance segmentation performance by prioritizing more accurate mask predictions during COCO AP evaluation. By extensive evaluations on the COCO dataset, Mask Scoring R-CNN brings consistent
    and noticeable gain with different models, and outperforms the state-of-the-art Mask R-CNN. We hope our simple and effective approach will provide a new direction for improving instance segmentation.
- algorithm: Instaboost
  title: 'Instaboost: Boosting instance segmentation via probability map guided copy-pasting'
  abstract: Instance segmentation requires a large number of training samples to achieve satisfactory performance and benefits from proper data augmentation. To enlarge the training set and increase the
    diversity, previous methods have investigated using data annotation from other domain (e.g. bbox, point) in a weakly supervised mechanism. In this paper, we present a simple, efficient and effective
    method to augment the training set using the existing instance mask annotations. Exploiting the pixel redundancy of the background, we are able to improve the performance of Mask R-CNN for 1.7 mAP on
    COCO dataset and 3.3 mAP on Pascal VOC dataset by simply introducing random jittering to objects. Furthermore, we propose a location probability map based approach to explore the feasible locations
    that objects can be placed based on local appearance similarity. With the guidance of such map, we boost the performance of R101-Mask R-CNN on instance segmentation from 35.7 mAP to 37.9 mAP without
    modifying the backbone or network structure. Our method is simple to implement and does not increase the computational complexity. It can be integrated into the training pipeline of any instance segmentation
    model without affecting the training and inference efficiency.
- algorithm: Swin
  title: 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows'
  abstract: This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language
    to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these
    differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation
    to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity
    with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction
    tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large
    margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach
    also prove beneficial for all-MLP architectures.
- algorithm: DCNv2
  title: 'Deformable ConvNets v2: More Deformable, Better Results'
  abstract: The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe
    that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing
    features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through
    increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation
    mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network
    to learn features that reflect the object focus and classification power of RCNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains
    over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation.
- algorithm: Conditional DETR
  title: Conditional DETR for Fast Training Convergence
  abstract: The DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training
    convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings and that
    the spatial embeddings make minor contributions, increasing the need for high-quality content embeddings and thus increasing the training difficulty.
- algorithm: GN
  title: Group Normalization
  abstract: Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems
    --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to
    computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative
    to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide
    range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and
    outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation
    in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.
- algorithm: MaskFormer
  title: Per-Pixel Classification is Not All You Need for Semantic Segmentation
  abstract: 'Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key
    insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following
    this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask
    classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer
    outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and
    panoptic segmentation (52.7 PQ on COCO) models.'
- algorithm: HRNet
  title: Deep High-Resolution Representation Learning for Human Pose Estimation
  abstract: 'This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with
    a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network.
    Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution
    subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations
    receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and
    spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and
    the MPII Human Pose dataset. High-resolution representation learning plays an essential role in many vision problems, e.g., pose estimation and semantic segmentation. The high-resolution network (HRNet),
    recently developed for human pose estimation, maintains high-resolution representations through the whole process by connecting high-to-low resolution convolutions in parallel and produces strong high-resolution
    representations by repeatedly conducting fusions across parallel convolutions. In this paper, we conduct a further study on high-resolution representations by introducing a simple yet effective modification
    and apply it to a wide range of vision tasks. We augment the high-resolution representation by aggregating the (upsampled) representations from all the parallel convolutions rather than only the representation
    from the high-resolution convolution as done in HRNet. This simple modification leads to stronger representations, evidenced by superior results. We show top results in semantic segmentation on Cityscapes,
    LIP, and PASCAL Context, and facial landmark detection on AFLW, COFW, 300W, and WFLW. In addition, we build a multi-level representation from the high-resolution representation and apply it to the Faster
    R-CNN object detection framework and the extended frameworks. The proposed approach achieves superior results to existing single-model networks on COCO object detection.'
- algorithm: DeepFashion
  title: 'DeepFashion: Powering Robust Clothes Recognition and Retrieval With Rich Annotations'
  abstract: Recent advances in clothes recognition have been driven by the construction of clothes datasets. Existing datasets are limited in the amount of annotations and are difficult to cope with the
    various challenges in real-world applications. In this work, we introduce DeepFashion, a large-scale clothes dataset with comprehensive annotations. It contains over 800,000 images, which are richly
    annotated with massive attributes, clothing landmarks, and correspondence of images taken under different scenarios including store, street snapshot, and consumer. Such rich annotations enable the development
    of powerful algorithms in clothes recognition and facilitating future researches. To demonstrate the advantages of DeepFashion, we propose a new deep model, namely FashionNet, which learns clothing
    features by jointly predicting clothing attributes and landmarks. The estimated landmarks are then employed to pool or gate the learned features. It is optimized in an iterative manner. Extensive experiments
    demonstrate the effectiveness of FashionNet and the usefulness of DeepFashion.
- algorithm: RepPoints
  title: 'RepPoints: Point Set Representation for Object Detection'
  abstract: Modern object detectors rely heavily on rectangular bounding boxes, such as anchors, proposals and the final predictions, to represent objects at various recognition stages. The bounding box
    is convenient to use but provides only a coarse localization of objects and leads to a correspondingly coarse extraction of object features. In this paper, we present RepPoints(representative points),
    a new finer representation of objects as a set of sample points useful for both localization and recognition. Given ground truth localization and recognition targets for training, RepPoints learn to
    automatically arrange themselves in a manner that bounds the spatial extent of an object and indicates semantically significant local areas. They furthermore do not require the use of anchors to sample
    a space of bounding boxes. We show that an anchor-free object detector based on RepPoints can be as effective as the state-of-the-art anchor-based detection methods, with 46.5 AP and 67.4 AP50 on the
    COCO test-dev detection benchmark, using ResNet-101 model.
- algorithm: LD
  title: Localization Distillation for Dense Object Detection
  abstract: Knowledge distillation (KD) has witnessed its powerful capability in learning compact models in object detection. Previous KD methods for object detection mostly focus on imitating deep features
    within the imitation regions instead of mimicking classification logits due to its inefficiency in distilling localization information. In this paper, by reformulating the knowledge distillation process
    on localization, we present a novel localization distillation (LD) method which can efficiently transfer the localization knowledge from the teacher to the student. Moreover, we also heuristically introduce
    the concept of valuable localization region that can aid to selectively distill the semantic and localization knowledge for a certain region. Combining these two new components, for the first time,
    we show that logit mimicking can outperform feature imitation and localization knowledge distillation is more important and efficient than semantic knowledge for distilling object detectors. Our distillation
    scheme is simple as well as effective and can be easily applied to different dense object detectors. Experiments show that our LD can boost the AP score of GFocal-ResNet-50 with a single-scale 1× training
    schedule from 40.1 to 42.1 on the COCO benchmark without any sacrifice on the inference speed.
- algorithm: YOLACT
  title: 'YOLACT: Real-time Instance Segmentation'
  abstract: 'We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster
    than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating
    a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this
    process doesn''t depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show
    they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS
    that only has a marginal performance penalty.'
- algorithm: DAB-DETR
  title: 'DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR'
  abstract: We present in this paper a novel query formulation using dynamic anchor boxes for DETR (DEtection TRansformer) and offer a deeper understanding of the role of queries in DETR. This new formulation
    directly uses box coordinates as queries in Transformer decoders and dynamically updates them layer-by-layer. Using box coordinates not only helps using explicit positional priors to improve the query-to-feature
    similarity and eliminate the slow training convergence issue in DETR, but also allows us to modulate the positional attention map using the box width and height information. Such a design makes it clear
    that queries in DETR can be implemented as performing soft ROI pooling layer-by-layer in a cascade manner. As a result, it leads to the best performance on MS-COCO benchmark among the DETR-like detection
    models under the same setting, e.g., AP 45.7% using ResNet50-DC5 as backbone trained in 50 epochs. We also conducted extensive experiments to confirm our analysis and verify the effectiveness of our
    methods.
- algorithm: Dynamic R-CNN
  title: 'Dynamic R-CNN: Towards High Quality Object Detection via Dynamic Training'
  abstract: Although two-stage object detectors have continuously advanced the state-of-the-art performance in recent years, the training process itself is far from crystal. In this work, we first point
    out the inconsistency problem between the fixed network settings and the dynamic training procedure, which greatly affects the performance. For example, the fixed label assignment strategy and regression
    loss function cannot fit the distribution change of proposals and thus are harmful to training high quality detectors. Consequently, we propose Dynamic R-CNN to adjust the label assignment criteria
    (IoU threshold) and the shape of regression loss function (parameters of SmoothL1 Loss) automatically based on the statistics of proposals during training. This dynamic design makes better use of the
    training samples and pushes the detector to fit more high quality samples. Specifically, our method improves upon ResNet-50-FPN baseline with 1.9% AP and 5.5% AP90 on the MS COCO dataset with no extra
    overhead.
- algorithm: ResNeSt
  title: 'ResNeSt: Split-Attention Networks'
  abstract: It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise
    attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation
    block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved
    superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge.
- algorithm: QueryInst
  title: Instances as Queries
  abstract: We present QueryInst, a new perspective for instance segmentation. QueryInst is a multi-stage end-to-end system that treats instances of interest as learnable queries, enabling query based object
    detectors, e.g., Sparse R-CNN, to have strong instance segmentation performance. The attributes of instances such as categories, bounding boxes, instance masks, and instance association embeddings are
    represented by queries in a unified manner. In QueryInst, a query is shared by both detection and segmentation via dynamic convolutions and driven by parallelly-supervised multi-stage learning. We conduct
    extensive experiments on three challenging benchmarks, i.e., COCO, CityScapes, and YouTube-VIS to evaluate the effectiveness of QueryInst in object detection, instance segmentation, and video instance
    segmentation tasks. For the first time, we demonstrate that a simple end-to-end query based framework can achieve the state-of-the-art performance in various instance-level recognition tasks.
- algorithm: LAD
  title: Improving Object Detection by Label Assignment Distillation
  abstract: 'Label assignment in object detection aims to assign targets, foreground or background, to sampled regions in an image. Unlike labeling for image classification, this problem is not well defined
    due to the object''s bounding box. In this paper, we investigate the problem from a perspective of distillation, hence we call Label Assignment Distillation (LAD). Our initial motivation is very simple,
    we use a teacher network to generate labels for the student. This can be achieved in two ways: either using the teacher''s prediction as the direct targets (soft label), or through the hard labels dynamically
    assigned by the teacher (LAD). Our experiments reveal that: (i) LAD is more effective than soft-label, but they are complementary. (ii) Using LAD, a smaller teacher can also improve a larger student
    significantly, while soft-label can''t. We then introduce Co-learning LAD, in which two networks simultaneously learn from scratch and the role of teacher and student are dynamically interchanged. Using
    PAA-ResNet50 as a teacher, our LAD techniques can improve detectors PAA-ResNet101 and PAA-ResNeXt101 to 46AP and 47.5AP on the COCO test-dev set. With a stronger teacher PAA-SwinB, we improve the students
    PAA-ResNet50 to 43.7AP by only 1x schedule training and standard setting, and PAA-ResNet101 to 47.9AP, significantly surpassing the current methods.'
- algorithm: DDQ
  title: Dense Distinct Query for End-to-End Object Detection
  abstract: <!-- [ABSTRACT] --> One-to-one label assignment in object detection has successfully obviated the need for non-maximum suppression (NMS) as postprocessing and makes the pipeline end-to-end.
    However, it triggers a new dilemma as the widely used sparse queries cannot guarantee a high recall, while dense queries inevitably bring more similar queries and encounter optimization difficulties.
    As both sparse and dense queries are problematic, then what are the expected queries in end-to-end object detection? This paper shows that the solution should be Dense Distinct Queries (DDQ). Concretely,
    we first lay dense queries like traditional detectors and then select distinct ones for one-to-one assignments. DDQ blends the advantages of traditional and recent end-to-end detectors and significantly
    improves the performance of various detectors including FCN, R-CNN, and DETRs. Most impressively, DDQ-DETR achieves 52.1 AP on MS-COCO dataset within 12 epochs using a ResNet-50 backbone, outperforming
    all existing detectors in the same setting. DDQ also shares the benefit of end-to-end detectors in crowded scenes and achieves 93.8 AP on CrowdHuman. We hope DDQ can inspire researchers to consider
    the complementarity between traditional methods and end-to-end detectors. ![ddq_arch](https://github.com/open-mmlab/mmdetection/assets/33146359/5ca9f11b-b6f3-454f-a2d1-3009ee337bbc)
- algorithm: PVT
  title: 'Pyramid vision transformer: A versatile backbone for dense prediction without convolutions'
  abstract: Although using convolutional neural networks (CNNs) as backbones achieves great successes in computer vision, this work investigates a simple backbone network useful for many dense prediction
    tasks without convolutions. Unlike the recently-proposed Transformer model (e.g., ViT) that is specially designed for image classification, we propose Pyramid Vision Transformer~(PVT), which overcomes
    the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to prior arts. (1) Different from ViT that typically has low-resolution outputs and high computational
    and memory cost, PVT can be not only trained on dense partitions of the image to achieve high output resolution, which is important for dense predictions but also using a progressive shrinking pyramid
    to reduce computations of large feature maps. (2) PVT inherits the advantages from both CNN and Transformer, making it a unified backbone in various vision tasks without convolutions by simply replacing
    CNN backbones. (3) We validate PVT by conducting extensive experiments, showing that it boosts the performance of many downstream tasks, e.g., object detection, semantic, and instance segmentation.
    For example, with a comparable number of parameters, RetinaNet+PVT achieves 40.4 AP on the COCO dataset, surpassing RetinNet+ResNet50 (36.3 AP) by 4.1 absolute AP. We hope PVT could serve as an alternative
    and useful backbone for pixel-level predictions and facilitate future researches. Transformer recently has shown encouraging progresses in computer vision. In this work, we present new baselines by
    improving the original Pyramid Vision Transformer (abbreviated as PVTv1) by adding three designs, including (1) overlapping patch embedding, (2) convolutional feed-forward networks, and (3) linear complexity
    attention layers. With these modifications, our PVTv2 significantly improves PVTv1 on three tasks e.g., classification, detection, and segmentation. Moreover, PVTv2 achieves comparable or better performances
    than recent works such as Swin Transformer. We hope this work will facilitate state-of-the-art Transformer researches in computer vision.
- algorithm: Empirical Attention
  title: An Empirical Study of Spatial Attention Mechanisms in Deep Networks
  abstract: Attention mechanisms have become a popular component in deep neural networks, yet there has been little examination of how different influencing factors and methods for computing attention from
    these factors affect performance. Toward a better general understanding of attention mechanisms, we present an empirical study that ablates various spatial attention elements within a generalized attention
    formulation, encompassing the dominant Transformer attention as well as the prevalent deformable convolution and dynamic convolution modules. Conducted on a variety of applications, the study yields
    significant findings about spatial attention in deep networks, some of which run counter to conventional understanding. For example, we find that the query and key content comparison in Transformer
    attention is negligible for self-attention, but vital for encoder-decoder attention. A proper combination of deformable convolution with key content only saliency achieves the best accuracy-efficiency
    tradeoff in self-attention. Our results suggest that there exists much room for improvement in the design of attention mechanisms.
- algorithm: RPN
  title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks'
  abstract: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection
    networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus
    enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to
    generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently
    popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of
    5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015
    competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks.
- algorithm: 'GLIP: Grounded Language-Image Pre-training'
  title: 'GLIP: Grounded Language-Image Pre-training'
  abstract: 'This paper presents a grounded language-image pre-training (GLIP) model for learning object-level, language-aware, and semantic-rich visual representations. GLIP unifies object detection and
    phrase grounding for pre-training. The unification brings two benefits: 1) it allows GLIP to learn from both detection and grounding data to improve both tasks and bootstrap a good grounding model;
    2) GLIP can leverage massive image-text pairs by generating grounding boxes in a self-training fashion, making the learned representation semantic-rich. In our experiments, we pre-train GLIP on 27M
    grounding data, including 3M human-annotated and 24M web-crawled image-text pairs. The learned representations demonstrate strong zero-shot and few-shot transferability to various object-level recognition
    tasks. 1) When directly evaluated on COCO and LVIS (without seeing any images in COCO during pre-training), GLIP achieves 49.8 AP and 26.9 AP, respectively, surpassing many supervised baselines. 2)
    After fine-tuned on COCO, GLIP achieves 60.8 AP on val and 61.5 AP on test-dev, surpassing prior SoTA. 3) When transferred to 13 downstream object detection tasks, a 1-shot GLIP rivals with a fully-supervised
    Dynamic Head.'
- algorithm: Mask2Former
  title: Masked-attention Mask Transformer for Universal Image Segmentation
  abstract: Image segmentation is about grouping pixels with different semantics, e.g., category or instance membership, where each choice of semantics defines a task. While only the semantics of each task
    differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image
    segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition
    to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art
    for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K).
- algorithm: Panoptic FPN
  title: Panoptic feature pyramid networks
  abstract: The recently introduced panoptic segmentation task has renewed our community's interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff
    classes). However, current state-of-the-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this
    work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic
    segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, top-performing
    method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and
    accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.
- algorithm: PAFPN
  title: Path Aggregation Network for Instance Segmentation
  abstract: The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based
    instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information
    path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly
    to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with
    subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It
    is also state-of-the-art on MVD and Cityscapes.
- algorithm: BoxInst
  title: 'BoxInst: High-Performance Instance Segmentation with Box Annotations'
  abstract: We present a high-performance method that can achieve mask-level instance segmentation with only bounding-box annotations for training. While this setting has been studied in the literature,
    here we show significantly stronger performance with a simple design (e.g., dramatically improving previous best reported mask AP of 21.1% to 31.6% on the COCO dataset). Our core idea is to redesign
    the loss of learning masks in instance segmentation, with no modification to the segmentation network itself. The new loss functions can supervise the mask training without relying on mask annotations.
    This is made possible with two loss terms, namely, 1) a surrogate term that minimizes the discrepancy between the projections of the ground-truth box and the predicted mask; 2) a pairwise loss that
    can exploit the prior that proximal pixels with similar colors are very likely to have the same category label. Experiments demonstrate that the redesigned mask loss can yield surprisingly high-quality
    instance masks with only box annotations. For example, without using any mask annotations, with a ResNet-101 backbone and 3× training schedule, we achieve 33.2% mask AP on COCO test-dev split (vs. 39.1%
    of the fully supervised counterpart). Our excellent experiment results on COCO and Pascal VOC indicate that our method dramatically narrows the performance gap between weakly and fully supervised instance
    segmentation.
- algorithm: Pascal VOC
  title: The Pascal Visual Object Classes (VOC) Challenge
  abstract: The Pascal Visual Object Classes (VOC) challenge is a benchmark in visual object category recognition and detection, providing the vision and machine learning communities with a standard dataset
    of images and annotation, and standard evaluation procedures. Organised annually from 2005 to present, the challenge and its associated dataset has become accepted as the benchmark for object detection.
    This paper describes the dataset and evaluation procedure. We review the state-of-the-art in evaluated methods for both classification and detection, analyse whether the methods are statistically different,
    what they are learning from the images (e.g. the object or its context), and what the methods find easy or confuse. The paper concludes with lessons learnt in the three year history of the challenge,
    and proposes directions for future improvement and extension.
- algorithm: CentripetalNet
  title: 'CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection'
  abstract: Keypoint-based detectors have achieved pretty-well performance. However, incorrect keypoint matching is still widespread and greatly affects the performance of the detector. In this paper, we
    propose CentripetalNet which uses centripetal shift to pair corner keypoints from the same instance. CentripetalNet predicts the position and the centripetal shift of the corner points and matches corners
    whose shifted results are aligned. Combining position information, our approach matches corner points more accurately than the conventional embedding approaches do. Corner pooling extracts information
    inside the bounding boxes onto the border. To make this information more aware at the corners, we design a cross-star deformable convolution network to conduct feature adaption. Furthermore, we explore
    instance segmentation on anchor-free detectors by equipping our CentripetalNet with a mask prediction module. On MS-COCO test-dev, our CentripetalNet not only outperforms all existing anchor-free detectors
    with an AP of 48.0% but also achieves comparable performance to the state-of-the-art instance segmentation approaches with a 40.2% MaskAP.
- algorithm: SCNet
  title: 'SCNet: Training Inference Sample Consistency for Instance Segmentation'
  abstract: <!-- [ABSTRACT] --> Cascaded architectures have brought significant performance improvement in object detection and instance segmentation. However, there are lingering issues regarding the disparity
    in the Intersection-over-Union (IoU) distribution of the samples between training and inference. This disparity can potentially exacerbate detection accuracy. This paper proposes an architecture referred
    to as Sample Consistency Network (SCNet) to ensure that the IoU distribution of the samples at training time is close to that at inference time. Furthermore, SCNet incorporates feature relay and utilizes
    global contextual information to further reinforce the reciprocal relationships among classifying, detecting, and segmenting sub-tasks. Extensive experiments on the standard COCO dataset reveal the
    effectiveness of the proposed method over multiple evaluation metrics, including box AP, mask AP, and inference speed. In particular, while running 38% faster, the proposed SCNet improves the AP of
    the box and mask predictions by respectively 1.3 and 2.3 points compared to the strong Cascade Mask R-CNN baseline.
- algorithm: SABL
  title: Side-Aware Boundary Localization for More Precise Object Detection
  abstract: Current object detection frameworks mainly rely on bounding box regression to localize objects. Despite the remarkable progress in recent years, the precision of bounding box regression remains
    unsatisfactory, hence limiting performance in object detection. We observe that precise localization requires careful placement of each side of the bounding box. However, the mainstream approach, which
    focuses on predicting centers and sizes, is not the most effective way to accomplish this task, especially when there exists displacements with large variance between the anchors and the targets. In
    this paper, we propose an alternative approach, named as Side-Aware Boundary Localization (SABL), where each side of the bounding box is respectively localized with a dedicated network branch. To tackle
    the difficulty of precise localization in the presence of displacements with large variance, we further propose a two-step localization scheme, which first predicts a range of movement through bucket
    prediction and then pinpoints the precise position within the predicted bucket. We test the proposed method on both two-stage and single-stage detection frameworks. Replacing the standard bounding box
    regression branch with the proposed design leads to significant improvements on Faster R-CNN, RetinaNet, and Cascade R-CNN, by 3.0%, 1.7%, and 0.9%, respectively.
- algorithm: FSAF
  title: Feature Selective Anchor-Free Module for Single-Shot Object Detection
  abstract: 'We motivate and present feature selective anchor-free (FSAF) module, a simple and effective building block for single-shot object detectors. It can be plugged into single-shot detectors with
    feature pyramid structure. The FSAF module addresses two limitations brought up by the conventional anchor-based detection: 1) heuristic-guided feature selection; 2) overlap-based anchor sampling. The
    general concept of the FSAF module is online feature selection applied to the training of multi-level anchor-free branches. Specifically, an anchor-free branch is attached to each level of the feature
    pyramid, allowing box encoding and decoding in the anchor-free manner at an arbitrary level. During training, we dynamically assign each instance to the most suitable feature level. At the time of inference,
    the FSAF module can work jointly with anchor-based branches by outputting predictions in parallel. We instantiate this concept with simple implementations of anchor-free branches and online feature
    selection strategy. Experimental results on the COCO detection track show that our FSAF module performs better than anchor-based counterparts while being faster. When working jointly with anchor-based
    branches, the FSAF module robustly improves the baseline RetinaNet by a large margin under various settings, while introducing nearly free inference overhead. And the resulting best model can achieve
    a state-of-the-art 44.6% mAP, outperforming all existing single-shot detectors on COCO.'
- algorithm: VarifocalNet
  title: 'VarifocalNet: An IoU-aware Dense Object Detector'
  abstract: Accurately ranking the vast number of candidate detections is crucial for dense object detectors to achieve high performance. Prior work uses the classification score or a combination of classification
    and predicted localization scores to rank candidates. However, neither option results in a reliable ranking, thus degrading detection performance. In this paper, we propose to learn an Iou-aware Classification
    Score (IACS) as a joint representation of object presence confidence and localization accuracy. We show that dense object detectors can achieve a more accurate ranking of candidate detections based
    on the IACS. We design a new loss function, named Varifocal Loss, to train a dense object detector to predict the IACS, and propose a new star-shaped bounding box feature representation for IACS prediction
    and bounding box refinement. Combining these two new components and a bounding box refinement branch, we build an IoU-aware dense object detector based on the FCOS+ATSS architecture, that we call VarifocalNet
    or VFNet for short. Extensive experiments on MS COCO show that our VFNet consistently surpasses the strong baseline by ∼2.0 AP with different backbones. Our best model VFNet-X-1200 with Res2Net-101-DCN
    achieves a single-model single-scale AP of 55.1 on COCO test-dev, which is state-of-the-art among various object detectors.
- algorithm: NAS-FPN
  title: 'NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection'
  abstract: Current state-of-the-art convolutional architectures for object detection are manually designed. Here we aim to learn a better architecture of feature pyramid network for object detection. We
    adopt Neural Architecture Search and discover a new feature pyramid architecture in a novel scalable search space covering all cross-scale connections. The discovered architecture, named NAS-FPN, consists
    of a combination of top-down and bottom-up connections to fuse features across scales. NAS-FPN, combined with various backbone models in the RetinaNet framework, achieves better accuracy and latency
    tradeoff compared to state-of-the-art object detection models. NAS-FPN improves mobile detection accuracy by 2 AP compared to state-of-the-art SSDLite with MobileNetV2 model in \[32\] and achieves 48.3
    AP which surpasses Mask R-CNN \[10\] detection accuracy with less computation time.
- algorithm: GCNet
  title: 'GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond'
  abstract: The Non-Local Network (NLNet) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous
    empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding
    to create a simplified network based on a query-independent formulation, which maintains the accuracy of NLNet but with significantly less computation. We further observe that this simplified design
    shares similar structure with Squeeze-Excitation Network (SENet). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better
    instantiation, called the global context (GC) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone
    network to construct a global context network (GCNet), which generally outperforms both simplified NLNet and SENet on major benchmarks for various recognition tasks.
- algorithm: PointRend
  title: 'PointRend: Image Segmentation as Rendering'
  abstract: 'We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling
    challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network
    module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and
    semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves
    excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes,
    for both instance and semantic segmentation. PointRend''s efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches.'
- algorithm: DINO
  title: 'DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection'
  abstract: We present DINO (DETR with Improved deNoising anchOr boxes), a state-of-the-art end-to-end object detector. DINO improves over previous DETR-like models in performance and efficiency by using
    a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves 49.4AP in 12 epochs and 51.3AP in
    24 epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of +6.0AP and +2.7AP, respectively, compared to DN-DETR, the previous best DETR-like model. DINO
    scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO val2017 (63.2AP)
    and test-dev (63.3AP). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results.
- algorithm: Open Images Dataset
  title: Open Images Dataset
  abstract: <!-- [ABSTRACT] -->
- algorithm: CARAFE
  title: 'CARAFE: Content-Aware ReAssembly of FEatures'
  abstract: 'Feature upsampling is a key operation in a number of modern convolutional network architectures, e.g. feature pyramids. Its design is critical for dense prediction tasks such as object detection
    and semantic/instance segmentation. In this work, we propose Content-Aware ReAssembly of FEatures (CARAFE), a universal, lightweight and highly effective operator to fulfill this goal. CARAFE has several
    appealing properties: (1) Large field of view. Unlike previous works (e.g. bilinear interpolation) that only exploit sub-pixel neighborhood, CARAFE can aggregate contextual information within a large
    receptive field. (2) Content-aware handling. Instead of using a fixed kernel for all samples (e.g. deconvolution), CARAFE enables instance-specific content-aware handling, which generates adaptive kernels
    on-the-fly. (3) Lightweight and fast to compute. CARAFE introduces little computational overhead and can be readily integrated into modern network architectures. We conduct comprehensive evaluations
    on standard benchmarks in object detection, instance/semantic segmentation and inpainting. CARAFE shows consistent and substantial gains across all the tasks (1.2%, 1.3%, 1.8%, 1.1db respectively) with
    negligible computational overhead. It has great potential to serve as a strong building block for future research. It has great potential to serve as a strong building block for future research.'
- algorithm: Deformable DETR
  title: 'Deformable DETR: Deformable Transformers for End-to-End Object Detection'
  abstract: DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and
    limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules
    only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive
    experiments on the COCO benchmark demonstrate the effectiveness of our approach.
- algorithm: Scratch
  title: Rethinking ImageNet Pre-training
  abstract: 'We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained from random initialization. The results are no worse than their
    ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine-tuning pre-trained models, with the sole exception of increasing
    the number of training iterations so the randomly initialized models may converge. Training from random initialization is surprisingly robust; our results hold even when: (i) using only 10% of the training
    data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics. Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide
    regularization or improve final target task accuracy. To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data---a result on par with the top COCO 2017 competition
    results that used ImageNet pre-training. These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink
    the current de facto paradigm of \`pre-training and fine-tuning'' in computer vision.'
- algorithm: GHM
  title: Gradient Harmonized Single-stage Detector
  abstract: Despite the great success of two-stage detectors, single-stage detector is still a more elegant and efficient way, yet suffers from the two well-known disharmonies during training, i.e. the
    huge difference in quantity between positive and negative examples as well as between easy and hard examples. In this work, we first point out that the essential effect of the two disharmonies can be
    summarized in term of the gradient. Further, we propose a novel gradient harmonizing mechanism (GHM) to be a hedging for the disharmonies. The philosophy behind GHM can be easily embedded into both
    classification loss function like cross-entropy (CE) and regression loss function like smooth-L1 (SL1) loss. To this end, two novel loss functions called GHM-C and GHM-R are designed to balancing the
    gradient flow for anchor classification and bounding box refinement, respectively. Ablation study on MS COCO demonstrates that without laborious hyper-parameter tuning, both GHM-C and GHM-R can bring
    substantial improvement for single-stage detector. Without any whistles and bells, our model achieves 41.6 mAP on COCO test-dev set which surpasses the state-of-the-art method, Focal Loss (FL) + SL1,
    by 0.8.
- algorithm: Sparse R-CNN
  title: 'Sparse R-CNN: End-to-End Object Detection with Learnable Proposals'
  abstract: We present Sparse R-CNN, a purely sparse method for object detection in images. Existing works on object detection heavily rely on dense object candidates, such as k anchor boxes pre-defined
    on all grids of image feature map of size H×W. In our method, however, a fixed sparse set of learned object proposals, total length of N, are provided to object recognition head to perform classification
    and location. By eliminating HWk (up to hundreds of thousands) hand-designed object candidates to N (e.g. 100) learnable proposals, Sparse R-CNN completely avoids all efforts related to object candidates
    design and many-to-one label assignment. More importantly, final predictions are directly output without non-maximum suppression post-procedure. Sparse R-CNN demonstrates accuracy, run-time and training
    convergence performance on par with the well-established detector baselines on the challenging COCO dataset, e.g., achieving 45.0 AP in standard 3× training schedule and running at 22 fps using ResNet-50
    FPN model. We hope our work could inspire re-thinking the convention of dense prior in object detectors.
- algorithm: SOLO
  title: 'SOLO: Segmenting Objects by Locations'
  abstract: We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of
    instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the 'detect-thensegment' strategy as used by Mask
    R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing
    the notion of "instance categories", which assigns categories to each pixel within an instance according to the instance's location and size, thus nicely converting instance mask segmentation into a
    classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance,
    achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level
    recognition tasks besides instance segmentation.
- algorithm: Cascade RPN
  title: 'Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution'
  abstract: This paper considers an architecture referred to as Cascade Region Proposal Network (Cascade RPN) for improving the region-proposal quality and detection performance by systematically addressing
    the limitation of the conventional RPN that heuristically defines the anchors and aligns the features to the anchors. First, instead of using multiple anchors with predefined scales and aspect ratios,
    Cascade RPN relies on a single anchor per location and performs multi-stage refinement. Each stage is progressively more stringent in defining positive samples by starting out with an anchor-free metric
    followed by anchor-based metrics in the ensuing stages. Second, to attain alignment between the features and the anchors throughout the stages, adaptive convolution is proposed that takes the anchors
    in addition to the image features as its input and learns the sampled features guided by the anchors. A simple implementation of a two-stage Cascade RPN achieves AR 13.4 points higher than that of the
    conventional RPN, surpassing any existing region proposal methods. When adopting to Fast R-CNN and Faster R-CNN, Cascade RPN can improve the detection mAP by 3.1 and 3.5 points, respectively.
- algorithm: FCOS
  title: 'FCOS: Fully Convolutional One-Stage Object Detection'
  abstract: We propose a fully convolutional one-stage object detector (FCOS) to solve object detection in a per-pixel prediction fashion, analogue to semantic segmentation. Almost all state-of-the-art
    object detectors such as RetinaNet, SSD, YOLOv3, and Faster R-CNN rely on pre-defined anchor boxes. In contrast, our proposed detector FCOS is anchor box free, as well as proposal free. By eliminating
    the predefined set of anchor boxes, FCOS completely avoids the complicated computation related to anchor boxes such as calculating overlapping during training. More importantly, we also avoid all hyper-parameters
    related to anchor boxes, which are often very sensitive to the final detection performance. With the only post-processing non-maximum suppression (NMS), FCOS with ResNeXt-64x4d-101 achieves 44.7% in
    AP with single-model and single-scale testing, surpassing previous one-stage detectors with the advantage of being much simpler. For the first time, we demonstrate a much simpler and flexible detection
    framework achieving improved detection accuracy. We hope that the proposed FCOS framework can serve as a simple and strong alternative for many other instance-level tasks.
- algorithm: Double Heads
  title: Rethinking Classification and Localization for Object Detection
  abstract: Two head structures (i.e. fully connected head and convolution head) have been widely used in R-CNN based detectors for classification and localization tasks. However, there is a lack of understanding
    of how does these two head structures work for these two tasks. To address this issue, we perform a thorough analysis and find an interesting fact that the two head structures have opposite preferences
    towards the two tasks. Specifically, the fully connected head (fc-head) is more suitable for the classification task, while the convolution head (conv-head) is more suitable for the localization task.
    Furthermore, we examine the output feature maps of both heads and find that fc-head has more spatial sensitivity than conv-head. Thus, fc-head has more capability to distinguish a complete object from
    part of an object, but is not robust to regress the whole object. Based upon these findings, we propose a Double-Head method, which has a fully connected head focusing on classification and a convolution
    head for bounding box regression. Without bells and whistles, our method gains +3.5 and +2.8 AP on MS COCO dataset from Feature Pyramid Network (FPN) baselines with ResNet-50 and ResNet-101 backbones,
    respectively.
- algorithm: DCN
  title: Deformable Convolutional Networks
  abstract: Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in its building modules. In this work, we introduce two new
    modules to enhance the transformation modeling capacity of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in
    the modules with additional offsets and learning the offsets from target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can
    be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the effectiveness of our approach on sophisticated vision
    tasks of object detection and semantic segmentation.
- algorithm: RetinaNet
  title: Focal Loss for Dense Object Detection
  abstract: The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast,
    one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus
    far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose
    to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse
    set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector
    we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art
    two-stage detectors.
- algorithm: 'RTMDet: An Empirical Study of Designing Real-Time Object Detectors'
  title: 'RTMDet: An Empirical Study of Designing Real-Time Object Detectors'
  abstract: In this paper, we aim to design an efficient real-time object detector that exceeds the YOLO series and is easily extensible for many object recognition tasks such as instance segmentation and
    rotated object detection. To obtain a more efficient model architecture, we explore an architecture that has compatible capacities in the backbone and neck, constructed by a basic building block that
    consists of large-kernel depth-wise convolutions. We further introduce soft labels when calculating matching costs in the dynamic label assignment to improve accuracy. Together with better training
    techniques, the resulting object detector, named RTMDet, achieves 52.8% AP on COCO with 300+ FPS on an NVIDIA 3090 GPU, outperforming the current mainstream industrial detectors. RTMDet achieves the
    best parameter-accuracy trade-off with tiny/small/medium/large/extra-large model sizes for various application scenarios, and obtains new state-of-the-art performance on real-time instance segmentation
    and rotated object detection. We hope the experimental results can provide new insights into designing versatile real-time object detectors for many object recognition tasks.
- algorithm: Grid R-CNN
  title: Grid R-CNN
  abstract: This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression
    based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points,
    we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points
    in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art
    detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared
    to Faster R-CNN with Res50 backbone and FPN architecture. Grid R-CNN is a well-performed objection detection framework. It transforms the traditional box offset regression problem into a grid point
    estimation problem. With the guidance of the grid points, it can obtain high-quality localization results. However, the speed of Grid R-CNN is not so satisfactory. In this technical report we present
    Grid R-CNN Plus, a better and faster version of Grid R-CNN. We have made several updates that significantly speed up the framework and simultaneously improve the accuracy. On COCO dataset, the Res50-FPN
    based Grid R-CNN Plus detector achieves an mAP of 40.4%, outperforming the baseline on the same model by 3.0 points with similar inference time.
- algorithm: Fast R-CNN
  title: Fast R-CNN
  abstract: This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep
    convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep
    VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate.
- algorithm: DyHead
  title: 'Dynamic Head: Unifying Object Detection Heads with Attentions'
  abstract: The complex nature of combining localization and classification in object detection has resulted in the flourished development of methods. Previous works tried to improve the performance in
    various object detection heads but failed to present a unified view. In this paper, we present a novel dynamic head framework to unify object detection heads with attentions. By coherently combining
    multiple self-attention mechanisms between feature levels for scale-awareness, among spatial locations for spatial-awareness, and within output channels for task-awareness, the proposed approach significantly
    improves the representation ability of object detection heads without any computational overhead. Further experiments demonstrate that the effectiveness and efficiency of the proposed dynamic head on
    the COCO benchmark. With a standard ResNeXt-101-DCN backbone, we largely improve the performance over popular object detectors and achieve a new state-of-the-art at 54.0 AP. Furthermore, with latest
    transformer backbone and extra data, we can push current best COCO result to a new record at 60.6 AP.
- algorithm: Objects365 Dataset
  title: Objects365 Dataset
  abstract: <!-- [ABSTRACT] -->
- algorithm: GN + WS
  title: Weight Standardization
  abstract: 'Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only
    1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight
    Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful
    elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients;
    BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN
    on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show
    that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.'
- algorithm: YOLOF
  title: You Only Look One-level Feature
  abstract: This paper revisits feature pyramids networks (FPN) for one-stage detectors and points out that the success of FPN is due to its divide-and-conquer solution to the optimization problem in object
    detection rather than multi-scale feature fusion. From the perspective of optimization, we introduce an alternative way to address the problem instead of adopting the complex feature pyramids - {\\em
    utilizing only one-level feature for detection}. Based on the simple and efficient solution, we present You Only Look One-level Feature (YOLOF). In our method, two key components, Dilated Encoder and
    Uniform Matching, are proposed and bring considerable improvements. Extensive experiments on the COCO benchmark prove the effectiveness of the proposed model. Our YOLOF achieves comparable results with
    its feature pyramids counterpart RetinaNet while being 2.5× faster. Without transformer layers, YOLOF can match the performance of DETR in a single-level feature manner with 7× less training epochs.
    With an image size of 608×608, YOLOF achieves 44.3 mAP running at 60 fps on 2080Ti, which is 13% faster than YOLOv4.
- algorithm: PISA
  title: Prime Sample Attention in Object Detection
  abstract: It is a common paradigm in object detection frameworks to treat all samples equally and target at maximizing the performance on average. In this work, we revisit this paradigm through a careful
    study on how different samples contribute to the overall performance measured in terms of mAP. Our study suggests that the samples in each mini-batch are neither independent nor equally important, and
    therefore a better classifier on average does not necessarily mean higher mAP. Motivated by this study, we propose the notion of Prime Samples, those that play a key role in driving the detection performance.
    We further develop a simple yet effective sampling and learning strategy called PrIme Sample Attention (PISA) that directs the focus of the training process towards such samples. Our experiments demonstrate
    that it is often more effective to focus on prime samples than hard samples when training a detector. Particularly, On the MSCOCO dataset, PISA outperforms the random sampling baseline and hard mining
    schemes, e.g., OHEM and Focal Loss, consistently by around 2% on both single-stage and two-stage detectors, even with a strong backbone ResNeXt-101.
- algorithm: DDOD
  title: Disentangle Your Dense Object Detector
  abstract: 'Deep learning-based dense object detectors have achieved great success in the past few years and have been applied to numerous multimedia applications such as video understanding. However,
    the current training pipeline for dense detectors is compromised to lots of conjunctions that may not hold. In this paper, we investigate three such important conjunctions: 1) only samples assigned
    as positive in classification head are used to train the regression head; 2) classification and regression share the same input feature and computational fields defined by the parallel head architecture;
    and 3) samples distributed in different feature pyramid layers are treated equally when computing the loss. We first carry out a series of pilot experiments to show disentangling such conjunctions can
    lead to persistent performance improvement. Then, based on these findings, we propose Disentangled Dense Object Detector(DDOD), in which simple and effective disentanglement mechanisms are designed
    and integrated into the current state-of-the-art dense object detectors. Extensive experiments on MS COCO benchmark show that our approach can lead to 2.0 mAP, 2.4 mAP and 2.2 mAP absolute improvements
    on RetinaNet, FCOS, and ATSS baselines with negligible extra overhead. Notably, our best model reaches 55.0 mAP on the COCO test-dev set and 93.5 AP on the hard subset of WIDER FACE, achieving new state-of-the-art
    performance on these two competitive benchmarks. Code is available at https://github.com/zehuichen123/DDOD.'
- algorithm: Res2Net
  title: 'Res2Net: A New Multi-scale Backbone Architecture'
  abstract: Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks (CNNs) continually demonstrate stronger multi-scale
    representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper,
    we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a
    granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and
    DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies
    and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the
    state-of-the-art baseline methods.
- algorithm: Albu Example
  title: 'Albumentations: fast and flexible image augmentations'
  abstract: Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve output labels. In computer
    vision domain, image augmentations have become a common implicit regularization technique to combat overfitting in deep convolutional neural networks and are ubiquitously used to improve performance.
    While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations and combinations of flipping, rotating, scaling, and cropping. Moreover, the
    image processing speed varies in existing tools for image augmentation. We present Albumentations, a fast and flexible library for image augmentations with many various image transform operations available,
    that is also an easy-to-use wrapper around other augmentation libraries. We provide examples of image augmentations for different computer vision tasks and show that Albumentations is faster than other
    commonly used image augmentation tools on the most of commonly used image transformations.
- algorithm: SOLOv2
  title: 'SOLOv2: Dynamic and Fast Instance Segmentation'
  abstract: 'In this work, we aim at building a simple, direct, and fast instance segmentation framework with strong performance. We follow the principle of the SOLO method of Wang et al. "SOLO: segmenting
    objects by locations". Importantly, we take one step further by dynamically learning the mask head of the object segmenter such that the mask head is conditioned on the location. Specifically, the mask
    branch is decoupled into a mask kernel branch and mask feature branch, which are responsible for learning the convolution kernel and the convolved features respectively. Moreover, we propose Matrix
    NMS (non maximum suppression) to significantly reduce the inference time overhead due to NMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in one shot, and yields better results.
    We demonstrate a simple direct instance segmentation system, outperforming a few state-of-the-art methods in both speed and accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields
    37.1% AP. Moreover, our state-of-the-art results in object detection (from our mask byproduct) and panoptic segmentation show the potential to serve as a new strong baseline for many instance-level
    recognition tasks besides instance segmentation.'
- algorithm: ResNet strikes back
  title: 'ResNet strikes back: An improved training procedure in timm'
  abstract: The influential Residual Networks designed by He et al. remain the gold-standard architecture in numerous scientific publications. They typically serve as the default architecture in studies,
    or as baselines when new architectures are proposed. Yet there has been significant progress on best practices for training neural networks since the inception of the ResNet architecture in 2015. Novel
    optimization & dataaugmentation have increased the effectiveness of the training recipes. In this paper, we re-evaluate the performance of the vanilla ResNet-50 when trained with a procedure that integrates
    such advances. We share competitive training settings and pre-trained models in the timm open-source library, with the hope that they will serve as better baselines for future work. For instance, with
    our more demanding training setting, a vanilla ResNet-50 reaches 80.4% top-1 accuracy at resolution 224×224 on ImageNet-val without extra data or distillation. We also report the performance achieved
    with popular models with our training procedure.
- algorithm: MM Grounding DINO
  title: An Open and Comprehensive Pipeline for Unified Object Grounding and Detection
  abstract: Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension
    (REC). Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications. However, despite its significance, the original Grounding-DINO model lacks
    comprehensive public technical details due to the unavailability of its training code. To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which
    is built with the MMDetection toolbox. It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning. We give a comprehensive analysis of each reported
    result and detailed settings for reproduction. The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline. We release
    all our models to the research community.
- algorithm: RegNet
  title: Designing Network Design Spaces
  abstract: 'In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead
    of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks,
    but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that
    we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design
    space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop
    regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.'
- algorithm: DetectoRS
  title: 'DetectoRS: Detecting Objects with Recursive Feature Pyramid and Switchable Atrous Convolution'
  abstract: Many modern object detectors demonstrate outstanding performances by using the mechanism of looking and thinking twice. In this paper, we explore this mechanism in the backbone design for object
    detection. At the macro level, we propose Recursive Feature Pyramid, which incorporates extra feedback connections from Feature Pyramid Networks into the bottom-up backbone layers. At the micro level,
    we propose Switchable Atrous Convolution, which convolves the features with different atrous rates and gathers the results using switch functions. Combining them results in DetectoRS, which significantly
    improves the performances of object detection. On COCO test-dev, DetectoRS achieves state-of-the-art 55.7% box AP for object detection, 48.5% mask AP for instance segmentation, and 50.0% PQ for panoptic
    segmentation.
- algorithm: CornerNet
  title: 'Cornernet: Detecting objects as paired keypoints'
  abstract: We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution
    neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation,
    we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2% AP on MS COCO, outperforming all existing one-stage
    detectors.
- algorithm: YOLOv3
  title: 'YOLOv3: An Incremental Improvement'
  abstract: We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more
    accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite
    good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster.
- algorithm: FreeAnchor
  title: 'FreeAnchor: Learning to Match Anchors for Visual Object Detection'
  abstract: Modern CNN-based object detectors assign anchors for ground-truth objects under the restriction of object-anchor Intersection-over-Unit (IoU). In this study, we propose a learning-to-match approach
    to break IoU restriction, allowing objects to match anchors in a flexible manner. Our approach, referred to as FreeAnchor, updates hand-crafted anchor assignment to "free" anchor matching by formulating
    detector training as a maximum likelihood estimation (MLE) procedure. FreeAnchor targets at learning features which best explain a class of objects in terms of both classification and localization.
    FreeAnchor is implemented by optimizing detection customized likelihood and can be fused with CNN-based detectors in a plug-and-play manner. Experiments on COCO demonstrate that FreeAnchor consistently
    outperforms their counterparts with significant margins.
- algorithm: DETR
  title: End-to-End Object Detection with Transformers
  abstract: We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed
    components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer
    or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons
    about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library,
    unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection
    dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines.
- algorithm: FoveaBox
  title: 'FoveaBox: Beyond Anchor-based Object Detector'
  abstract: 'We present FoveaBox, an accurate, flexible, and completely anchor-free framework for object detection. While almost all state-of-the-art object detectors utilize predefined anchors to enumerate
    possible locations, scales and aspect ratios for the search of the objects, their performance and generalization ability are also limited to the design of anchors. Instead, FoveaBox directly learns
    the object existing possibility and the bounding box coordinates without anchor reference. This is achieved by: (a) predicting category-sensitive semantic maps for the object existing possibility, and
    (b) producing category-agnostic bounding box for each position that potentially contains an object. The scales of target boxes are naturally associated with feature pyramid representations. In FoveaBox,
    an instance is assigned to adjacent feature levels to make the model more accurate.We demonstrate its effectiveness on standard benchmarks and report extensive experimental analysis. Without bells and
    whistles, FoveaBox achieves state-of-the-art single model performance on the standard COCO and Pascal VOC object detection benchmark. More importantly, FoveaBox avoids all computation and hyper-parameters
    related to anchor boxes, which are often sensitive to the final detection performance. We believe the simple and effective approach will serve as a solid baseline and help ease future research for object
    detection.'
- algorithm: Cityscapes
  title: The Cityscapes Dataset for Semantic Urban Scene Understanding
  abstract: Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in
    the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes,
    a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences
    recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes
    of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an
    in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.
- algorithm: PAA
  title: Probabilistic Anchor Assignment with IoU Prediction for Object Detection
  abstract: In object detection, determining which anchors to assign as positive or negative samples, known as anchor assignment, has been revealed as a core procedure that can significantly affect a model's
    performance. In this paper we propose a novel anchor assignment strategy that adaptively separates anchors into positive and negative samples for a ground truth bounding box according to the model's
    learning status such that it is able to reason about the separation in a probabilistic manner. To do so we first calculate the scores of anchors conditioned on the model and fit a probability distribution
    to these scores. The model is then trained with anchors separated into positive and negative samples according to their probabilities. Moreover, we investigate the gap between the training and testing
    objectives and propose to predict the Intersection-over-Unions of detected boxes as a measure of localization quality to reduce the discrepancy. The combined score of classification and localization
    qualities serving as a box selection metric in non-maximum suppression well aligns with the proposed anchor assignment strategy and leads significant performance improvements. The proposed methods only
    add a single convolutional layer to RetinaNet baseline and does not require multiple anchors per location, so are efficient. Experimental results verify the effectiveness of the proposed methods. Especially,
    our models set new records for single-stage detectors on MS COCO test-dev dataset with various backbones.
- algorithm: Seesaw Loss
  title: Seesaw Loss for Long-Tailed Instance Segmentation
  abstract: Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of
    objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples
    on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem,
    we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The mitigation
    factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified
    instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training
    pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles.
- algorithm: WIDER FACE
  title: 'WIDER FACE: A Face Detection Benchmark'
  abstract: Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that
    there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset, which is 10 times larger
    than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to
    large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative
    detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further
    investigated.
- algorithm: TOOD
  title: 'TOOD: Task-aligned One-stage Object Detection'
  abstract: 'One-stage object detection is commonly implemented by optimizing two sub-tasks: object classification and localization, using heads with two parallel branches, which might lead to a certain
    level of spatial misalignment in predictions between the two tasks. In this work, we propose a Task-aligned One-stage Object Detection (TOOD) that explicitly aligns the two tasks in a learning-based
    manner. First, we design a novel Task-aligned Head (T-Head) which offers a better balance between learning task-interactive and task-specific features, as well as a greater flexibility to learn the
    alignment via a task-aligned predictor. Second, we propose Task Alignment Learning (TAL) to explicitly pull closer (or even unify) the optimal anchors for the two tasks during training via a designed
    sample assignment scheme and a task-aligned loss. Extensive experiments are conducted on MS-COCO, where TOOD achieves a 51.1 AP at single-model single-scale testing. This surpasses the recent one-stage
    detectors by a large margin, such as ATSS (47.7 AP), GFL (48.2 AP), and PAA (49.0 AP), with fewer parameters and FLOPs. Qualitative results also demonstrate the effectiveness of TOOD for better aligning
    the tasks of object classification and localization.'
- algorithm: FPG
  title: Feature Pyramid Grids
  abstract: Feature pyramid networks have been widely adopted in the object detection literature to improve feature representations for better handling of variations in scale. In this paper, we present
    Feature Pyramid Grids (FPG), a deep multi-pathway feature pyramid, that represents the feature scale-space as a regular grid of parallel bottom-up pathways which are fused by multi-directional lateral
    connections. FPG can improve single-pathway feature pyramid networks by significantly increasing its performance at similar computation cost, highlighting importance of deep pyramid representations.
    In addition to its general and uniform structure, over complicated structures that have been found with neural architecture search, it also compares favorably against such approaches without relying
    on search. We hope that FPG with its uniform and effective nature can serve as a strong component for future work in object recognition.
- algorithm: ATSS
  title: Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection
  abstract: Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper,
    we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between
    them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows
    that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and
    negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we
    discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the
    newly introduced ATSS, we improve state-of-the-art detectors by a large margin to 50.7% AP without introducing any overhead.
- algorithm: AutoAssign
  title: 'AutoAssign: Differentiable Label Assignment for Dense Object Detection'
  abstract: Determining positive/negative samples for object detection is known as label assignment. Here we present an anchor-free detector named AutoAssign. It requires little human knowledge and achieves
    appearance-aware through a fully differentiable weighting mechanism. During training, to both satisfy the prior distribution of data and adapt to category characteristics, we present Center Weighting
    to adjust the category-specific prior distributions. To adapt to object appearances, Confidence Weighting is proposed to adjust the specific assign strategy of each instance. The two weighting modules
    are then combined to generate positive and negative weights to adjust each location's confidence. Extensive experiments on the MS COCO show that our method steadily surpasses other best sampling strategies
    by large margins with various backbones. Moreover, our best model achieves 52.1% AP, outperforming all existing one-stage detectors. Besides, experiments on other datasets, e.g., PASCAL VOC, Objects365,
    and WiderFace, demonstrate the broad applicability of AutoAssign.
- algorithm: SimpleCopyPaste
  title: Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation
  abstract: Building instance segmentation models that are data-efficient and can handle rare object categories is an important challenge in computer vision. Leveraging data augmentations is a promising
    direction towards addressing this challenge. Here, we perform a systematic study of the Copy-Paste augmentation (\[13, 12\]) for instance segmentation where we randomly paste objects onto an image.
    Prior studies on Copy-Paste relied on modeling the surrounding visual context for pasting the objects. However, we find that the simple mechanism of pasting objects randomly is good enough and can provide
    solid gains on top of strong baselines. Furthermore, we show Copy-Paste is additive with semi-supervised methods that leverage extra data through pseudo labeling (e.g. self-training). On COCO instance
    segmentation, we achieve 49.1 mask AP and 57.3 box AP, an improvement of +0.6 mask AP and +1.5 box AP over the previous state-of-the-art. We further demonstrate that Copy-Paste can lead to significant
    improvements on the LVIS benchmark. Our baseline model outperforms the LVIS 2020 Challenge winning entry by +3.6 mask AP on rare categories.
- algorithm: NAS-FCOS
  title: 'NAS-FCOS: Fast Neural Architecture Search for Object Detection'
  abstract: The success of deep neural networks relies on significant architecture engineering. Recently neural architecture search (NAS) has emerged as a promise to greatly reduce manual effort in network
    design by automatically searching for optimal architectures, although typically such algorithms need an excessive amount of computational resources, e.g., a few thousand GPU-days. To date, on challenging
    vision tasks such as object detection, NAS, especially fast versions of NAS, is less studied. Here we propose to search for the decoder structure of object detectors with search efficiency being taken
    into consideration. To be more specific, we aim to efficiently search for the feature pyramid network (FPN) as well as the prediction head of a simple anchor-free object detector, namely FCOS, using
    a tailored reinforcement learning paradigm. With carefully designed search space, search algorithms and strategies for evaluating network quality, we are able to efficiently search a top-performing
    detection architecture within 4 days using 8 V100 GPUs. The discovered architecture surpasses state-of-the-art object detection models (such as Faster R-CNN, RetinaNet and FCOS) by 1.5 to 3.5 points
    in AP on the COCO dataset, with comparable computation complexity and memory footprint, demonstrating the efficacy of the proposed NAS for object detection.
- algorithm: Timm Example
  title: PyTorch Image Models
  abstract: Py**T**orch **Im**age **M**odels (`timm`) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts
    that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results. <!--
- algorithm: GFL
  title: 'Generalized Focal Loss: Learning Qualified and Distributed Bounding Boxes for Dense Object Detection'
  abstract: 'One-stage detector basically formulates object detection as dense classification and localization. The classification is usually optimized by Focal Loss and the box location is commonly learned
    under Dirac delta distribution. A recent trend for one-stage detectors is to introduce an individual prediction branch to estimate the quality of localization, where the predicted quality facilitates
    the classification to improve detection performance. This paper delves into the representations of the above three fundamental elements: quality estimation, classification and localization. Two problems
    are discovered in existing practices, including (1) the inconsistent usage of the quality estimation and classification between training and inference and (2) the inflexible Dirac delta distribution
    for localization when there is ambiguity and uncertainty in complex scenes. To address the problems, we design new representations for these elements. Specifically, we merge the quality estimation into
    the class prediction vector to form a joint representation of localization quality and classification, and use a vector to represent arbitrary distribution of box locations. The improved representations
    eliminate the inconsistency risk and accurately depict the flexible distribution in real data, but contain continuous labels, which is beyond the scope of Focal Loss. We then propose Generalized Focal
    Loss (GFL) that generalizes Focal Loss from its discrete form to the continuous version for successful optimization. On COCO test-dev, GFL achieves 45.0% AP using ResNet-101 backbone, surpassing state-of-the-art
    SAPD (43.5%) and ATSS (43.6%) with higher or comparable inference speed, under the same backbone and training settings. Notably, our best model can achieve a single-model single-scale AP of 48.2%, at
    10 FPS on a single 2080Ti GPU.'
- algorithm: SoftTeacher
  title: End-to-End Semi-Supervised Object Detection with Soft Teacher
  abstract: 'This paper presents an end-to-end semi-supervised object detection approach, in contrast to previous more complex multi-stage methods. The end-to-end training gradually improves pseudo label
    qualities during the curriculum, and the more and more accurate pseudo labels in turn benefit object detection training. We also propose two simple yet effective techniques within this framework: a
    soft teacher mechanism where the classification loss of each unlabeled bounding box is weighed by the classification score produced by the teacher network; a box jittering approach to select reliable
    pseudo boxes for the learning of box regression. On the COCO benchmark, the proposed approach outperforms previous methods by a large margin under various labeling ratios, i.e. 1%, 5% and 10%. Moreover,
    our approach proves to perform also well when the amount of labeled data is relatively large. For example, it can improve a 40.9 mAP baseline detector trained using the full COCO training set by +3.6
    mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev), it can still significantly improve the
    detection accuracy by +1.5 mAP, reaching 60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching 52.4 mAP. Further incorporating with the Object365 pre-trained model, the detection
    accuracy reaches 61.3 mAP and the instance segmentation accuracy reaches 53.0 mAP, pushing the new state-of-the-art.'
- title: 'CondInst: Conditional Convolutions for Instance Segmentation'
  algorithm: CondInst
  abstract: 'We propose a simple yet effective instance segmentation framework, termed CondInst (conditional convolutions for instance segmentation). Top-performing instance segmentation methods such as
    Mask R-CNN rely on ROI operations (typically ROIPool or ROIAlign) to obtain the final instance masks. In contrast, we propose to solve instance segmentation from a new perspective. Instead of using
    instance-wise ROIs as inputs to a network of fixed weights, we employ dynamic instance-aware networks, conditioned on instances. CondInst enjoys two advantages: 1) Instance segmentation is solved by
    a fully convolutional network, eliminating the need for ROI cropping and feature alignment. 2) Due to the much improved capacity of dynamically-generated conditional convolutions, the mask head can
    be very compact (e.g., 3 conv. layers, each having only 8 channels), leading to significantly faster inference. We demonstrate a simpler instance segmentation method that can achieve improved performance
    in both accuracy and inference speed. On the COCO dataset, we outperform a few recent methods including well-tuned Mask R-CNN baselines, without longer training schedules needed.'
- title: 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks'
  algorithm: EfficientNet
  abstract: Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically
    study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly
    scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even
    further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than
    previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.
    Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.
- title: 'Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection'
  algorithm: 'Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection'
  abstract: In this paper, we present an open-set object detector, called Grounding DINO, by marrying Transformer-based detector DINO with grounded pre-training, which can detect arbitrary objects with
    human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector for open-set concept generalization. To effectively
    fuse language and vision modalities, we conceptually divide a closed-set detector into three phases and propose a tight fusion solution, which includes a feature enhancer, a language-guided query selection,
    and a cross-modality decoder for cross-modality fusion. While previous works mainly evaluate open-set object detection on novel categories, we propose to also perform evaluations on referring expression
    comprehension for objects specified with attributes. Grounding DINO performs remarkably well on all three settings, including benchmarks on COCO, LVIS, ODinW, and RefCOCO/+/g. Grounding DINO achieves
    a 52.5 AP on the COCO detection zero-shot transfer benchmark, i.e., without any training data from COCO. It sets a new record on the ODinW zero-shot benchmark with a mean 26.1 AP.
metrics:
  mmdetection:
  - repository: mmdetection
    file: configs/selfsup_pretrain/README.md
    config: selfsup_pretrain
    algorithm: Backbones Trained by Self-Supervise Algorithms
    models:
    - model: R50 by MoCo v2
      style: pytorch
      lr_schedule: 1x
      box_AP: 38.0
      mask_AP: 34.3
    - model: R50 by MoCo v2
      style: pytorch
      lr_schedule: multi-scale 2x
      box_AP: 40.8
      mask_AP: 36.8
    - model: R50 by SwAV
      style: pytorch
      lr_schedule: 1x
      box_AP: 39.1
      mask_AP: 35.7
    - model: R50 by SwAV
      style: pytorch
      lr_schedule: multi-scale 2x
      box_AP: 41.3
      mask_AP: 37.3
  - repository: mmdetection
    file: configs/libra_rcnn/README.md
    config: libra_rcnn
    algorithm: Libra R-CNN
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.6
      fps: 19.0
      box_AP: 38.3
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.5
      fps: 14.4
      box_AP: 40.1
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.8
      fps: 8.5
      box_AP: 42.7
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.2
      fps: 17.7
      box_AP: 37.6
  - repository: mmdetection
    file: configs/lvis/README.md
    config: lvis
    algorithm: LVIS
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 26.1
      mask_AP: 25.9
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 27.1
      mask_AP: 27.0
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 26.7
      mask_AP: 26.9
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 26.4
      mask_AP: 26.0
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.1
      box_AP: 22.5
      mask_AP: 21.7
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.8
      box_AP: 24.6
      mask_AP: 23.6
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 11.8
      box_AP: 26.7
      mask_AP: 25.5
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 14.6
      box_AP: 27.2
      mask_AP: 25.8
  - repository: mmdetection
    file: configs/crowddet/README.md
    config: crowddet
    algorithm: CrowdDet
    models:
    - model: R-50-FPN
      style: pytorch
      mem_gb: 4.4
      box_AP: 90.0
    - model: R-50-FPN
      style: pytorch
      mem_gb: 4.8
      box_AP: 90.32
  - repository: mmdetection
    file: configs/ms_rcnn/README.md
    config: ms_rcnn
    algorithm: MS R-CNN
    models:
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 4.5
      box_AP: 38.2
      mask_AP: 36.0
    - model: R-50-FPN
      style: caffe
      lr_schedule: 2x
      box_AP: 38.8
      mask_AP: 36.3
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 6.5
      box_AP: 40.4
      mask_AP: 37.6
    - model: R-101-FPN
      style: caffe
      lr_schedule: 2x
      box_AP: 41.1
      mask_AP: 38.1
    - model: R-X101-32x4d
      style: pytorch
      lr_schedule: 2x
      mem_gb: 7.9
      fps: 11.0
      box_AP: 41.8
      mask_AP: 38.7
    - model: R-X101-64x4d
      style: pytorch
      lr_schedule: 1x
      mem_gb: 11.0
      fps: 8.0
      box_AP: 43.0
      mask_AP: 39.5
    - model: R-X101-64x4d
      style: pytorch
      lr_schedule: 2x
      mem_gb: 11.0
      fps: 8.0
      box_AP: 42.6
      mask_AP: 39.5
  - repository: mmdetection
    file: configs/instaboost/README.md
    config: instaboost
    algorithm: Instaboost
    models:
    - model: R-50-FPN
      lr_schedule: 4x
      mem_gb: 4.4
      fps: 17.5
      box_AP: 40.6
      mask_AP: 36.6
    - model: R-101-FPN
      lr_schedule: 4x
      mem_gb: 6.4
      box_AP: 42.5
      mask_AP: 38.0
    - model: X-101-64x4d-FPN
      lr_schedule: 4x
      mem_gb: 10.7
      box_AP: 44.7
      mask_AP: 39.7
    - model: R-101-FPN
      lr_schedule: 4x
      mem_gb: 6.0
      fps: 12.0
      box_AP: 43.7
      mask_AP: 38.0
  - repository: mmdetection
    file: configs/ssd/README.md
    config: ssd
    algorithm: SSD
    models:
    - model: VGG16
      size: 300.0
      style: caffe
      lr_schedule: 120e
      mem_gb: 9.9
      fps: 43.7
      box_AP: 25.5
    - model: VGG16
      size: 512.0
      style: caffe
      lr_schedule: 120e
      mem_gb: 19.4
      fps: 30.7
      box_AP: 29.5
    - model: MobileNetV2
      size: 320.0
      lr_schedule: 600e
      mem_gb: 4.0
      fps: 69.9
      box_AP: 21.3
  - repository: mmdetection
    file: configs/htc/README.md
    config: htc
    algorithm: HTC
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.2
      fps: 5.8
      box_AP: 42.3
      mask_AP: 37.4
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 8.2
      box_AP: 43.3
      mask_AP: 38.3
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 10.2
      fps: 5.5
      box_AP: 44.8
      mask_AP: 39.6
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 11.4
      fps: 5.0
      box_AP: 46.1
      mask_AP: 40.5
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 14.5
      fps: 4.4
      box_AP: 47.0
      mask_AP: 41.4
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 20e
      box_AP: 50.4
      mask_AP: 43.8
  - repository: mmdetection
    file: configs/groie/README.md
    config: groie
    algorithm: GRoIE
    models:
    - model: Faster Original
      lr_schedule: 1x
      box_AP: 37.4
    - model: + GRoIE
      lr_schedule: 1x
      box_AP: 38.3
    - model: Grid R-CNN
      lr_schedule: 1x
      box_AP: 39.1
    - model: + GRoIE
      lr_schedule: 1x
    - model: Mask R-CNN
      lr_schedule: 1x
      box_AP: 38.2
      mask_AP: 34.7
    - model: + GRoIE
      lr_schedule: 1x
      box_AP: 39.0
      mask_AP: 36.0
    - model: GC-Net
      lr_schedule: 1x
      box_AP: 40.7
      mask_AP: 36.5
    - model: + GRoIE
      lr_schedule: 1x
      box_AP: 41.0
      mask_AP: 37.8
    - model: GC-Net
      lr_schedule: 1x
      box_AP: 42.2
      mask_AP: 37.8
    - model: + GRoIE
      lr_schedule: 1x
      box_AP: 42.6
      mask_AP: 38.7
  - repository: mmdetection
    file: configs/guided_anchoring/README.md
    config: guided_anchoring
    algorithm: Guided Anchoring
    models:
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.5
      box_AP: 39.6
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 7.5
      box_AP: 41.5
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.7
      fps: 9.7
      box_AP: 43.0
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 11.8
      fps: 7.3
      box_AP: 43.9
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.5
      fps: 16.8
      box_AP: 36.9
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.5
      fps: 12.9
      box_AP: 39.0
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.9
      fps: 10.6
      box_AP: 40.5
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.9
      fps: 7.7
      box_AP: 41.3
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      score_thr: 0.05
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      score_thr: 0.001
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      score_thr: 0.05
    - model: R-101-FPN
      style: caffe
      lr_schedule: 2x
      score_thr: 0.05
  - repository: mmdetection
    file: configs/strong_baselines/README.md
    config: strong_baselines
    algorithm: Strong Baselines
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 50e
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 100e
    - model: R-50-FPN
      style: caffe
      lr_schedule: 100e
      box_AP: 44.7
      mask_AP: 40.4
    - model: R-50-FPN
      style: caffe
      lr_schedule: 400e
  - repository: mmdetection
    file: configs/centernet/README.md
    config: centernet
    algorithm: CenterNet
    models:
    - model: ResNet-18
      mem_gb: 3.45
      box_AP: 27.3
    - model: ResNet-18
      mem_gb: 3.47
      box_AP: 30.9
    - model: ResNet-50
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.3
      box_AP: 40.2
  - repository: mmdetection
    file: configs/swin/README.md
    config: swin
    algorithm: Swin
    models:
    - model: Swin-T
      lr_schedule: 1x
      mem_gb: 7.6
      box_AP: 42.7
      mask_AP: 39.3
    - model: Swin-T
      lr_schedule: 3x
      mem_gb: 10.2
      box_AP: 46.0
      mask_AP: 41.6
    - model: Swin-T
      lr_schedule: 3x
      mem_gb: 7.8
      box_AP: 46.0
      mask_AP: 41.7
    - model: Swin-S
      lr_schedule: 3x
      mem_gb: 11.9
      box_AP: 48.2
      mask_AP: 43.2
  - repository: mmdetection
    file: configs/yolox/README.md
    config: yolox
    algorithm: YOLOX
    models:
    - model: YOLOX-tiny
      size: 416.0
      mem_gb: 3.5
      box_AP: 32.0
    - model: YOLOX-s
      size: 640.0
      mem_gb: 7.6
      box_AP: 40.5
    - model: YOLOX-l
      size: 640.0
      mem_gb: 19.9
      box_AP: 49.4
    - model: YOLOX-x
      size: 640.0
      mem_gb: 28.1
      box_AP: 50.9
  - repository: mmdetection
    file: configs/convnext/README.md
    config: convnext
    algorithm: ConvNeXt
    models:
    - model: ConvNeXt-T
      lr_schedule: 3x
      mem_gb: 7.3
      box_AP: 46.2
      mask_AP: 41.7
    - model: ConvNeXt-T
      lr_schedule: 3x
      mem_gb: 9.0
      box_AP: 50.3
      mask_AP: 43.6
    - model: ConvNeXt-S
      lr_schedule: 3x
      mem_gb: 12.3
      box_AP: 51.8
      mask_AP: 44.8
  - repository: mmdetection
    file: configs/dcnv2/README.md
    config: dcnv2
    algorithm: DCNv2
    models:
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.1
      fps: 17.6
      box_AP: 41.4
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.2
      fps: 17.4
      box_AP: 41.5
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.8
      fps: 16.6
      box_AP: 38.7
    - model: Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.5
      fps: 15.1
      box_AP: 41.5
      mask_AP: 37.1
    - model: Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.1
      box_AP: 42.0
      mask_AP: 37.6
  - repository: mmdetection
    file: configs/tridentnet/README.md
    config: tridentnet
    algorithm: TridentNet
    models:
    - model: R-50
      style: caffe
      lr_schedule: 1x
      box_AP: 37.7
    - model: R-50
      style: caffe
      lr_schedule: 1x
      box_AP: 37.6
    - model: R-50
      style: caffe
      lr_schedule: 3x
      box_AP: 40.3
  - repository: mmdetection
    file: configs/conditional_detr/README.md
    config: conditional_detr
    algorithm: Conditional DETR
    models:
    - model: Conditional DETR
      lr_schedule: 50e
      box_AP: 41.1
  - repository: mmdetection
    file: configs/gn/README.md
    config: gn
    algorithm: GN
    models:
    - model: Mask R-CNN
      lr_schedule: 2x
      mem_gb: 7.1
      fps: 11.0
      box_AP: 40.2
      mask_AP: 36.4
    - model: Mask R-CNN
      lr_schedule: 3x
      mem_gb: 7.1
      box_AP: 40.5
      mask_AP: 36.7
    - model: Mask R-CNN
      lr_schedule: 2x
      mem_gb: 9.9
      fps: 9.0
      box_AP: 41.9
      mask_AP: 37.6
    - model: Mask R-CNN
      lr_schedule: 3x
      mem_gb: 9.9
      box_AP: 42.1
      mask_AP: 38.0
    - model: Mask R-CNN
      lr_schedule: 2x
      mem_gb: 7.1
      fps: 10.9
      box_AP: 40.0
      mask_AP: 36.1
    - model: Mask R-CNN
      lr_schedule: 3x
      mem_gb: 7.1
      box_AP: 40.1
      mask_AP: 36.2
  - repository: mmdetection
    file: configs/hrnet/README.md
    config: hrnet
    algorithm: HRNet
    models:
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.6
      fps: 13.4
      box_AP: 36.9
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 2x
      mem_gb: 6.6
      box_AP: 38.9
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.0
      fps: 12.4
      box_AP: 40.2
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 2x
      mem_gb: 9.0
      box_AP: 41.4
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.4
      fps: 10.5
      box_AP: 41.2
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 2x
      mem_gb: 10.4
      box_AP: 42.1
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.0
      fps: 11.7
      box_AP: 37.7
      mask_AP: 34.2
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 2x
      mem_gb: 7.0
      box_AP: 39.8
      mask_AP: 36.0
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.4
      fps: 11.3
      box_AP: 41.2
      mask_AP: 37.1
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 2x
      mem_gb: 9.4
      box_AP: 42.5
      mask_AP: 37.8
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.9
      box_AP: 42.1
      mask_AP: 37.5
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 2x
      mem_gb: 10.9
      box_AP: 42.8
      mask_AP: 38.2
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 20e
      mem_gb: 7.0
      fps: 11.0
      box_AP: 41.2
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 20e
      mem_gb: 9.4
      fps: 11.0
      box_AP: 43.3
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 20e
      mem_gb: 10.8
      box_AP: 43.8
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 20e
      mem_gb: 8.5
      fps: 8.5
      box_AP: 41.6
      mask_AP: 36.4
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 20e
      fps: 8.3
      box_AP: 44.3
      mask_AP: 38.6
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 20e
      mem_gb: 12.5
      box_AP: 45.1
      mask_AP: 39.3
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 20e
      mem_gb: 10.8
      fps: 4.7
      box_AP: 42.8
      mask_AP: 37.9
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 20e
      mem_gb: 13.1
      fps: 4.9
      box_AP: 45.4
      mask_AP: 39.9
    - model: HRNetV2p-W40
      style: pytorch
      lr_schedule: 20e
      mem_gb: 14.6
      box_AP: 46.4
      mask_AP: 40.8
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 1x
      mem_gb: 13.0
      fps: 12.9
      box_AP: 35.3
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 2x
      mem_gb: 13.0
      box_AP: 38.2
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 1x
      mem_gb: 17.5
      fps: 12.9
      box_AP: 39.5
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 2x
      mem_gb: 17.5
      box_AP: 40.8
    - model: HRNetV2p-W18
      style: pytorch
      lr_schedule: 2x
      mem_gb: 13.0
      fps: 12.9
      box_AP: 38.3
    - model: HRNetV2p-W32
      style: pytorch
      lr_schedule: 2x
      mem_gb: 17.5
      fps: 12.4
      box_AP: 41.9
    - model: HRNetV2p-W48
      style: pytorch
      lr_schedule: 2x
      mem_gb: 20.3
      fps: 10.8
      box_AP: 42.7
  - repository: mmdetection
    file: configs/reppoints/README.md
    config: reppoints
    algorithm: RepPoints
    models:
    - model: R-50-FPN
      lr_schedule: 1x
      mem_gb: 3.9
      fps: 15.9
      box_AP: 36.4
    - model: R-50-FPN
      lr_schedule: 1x
      mem_gb: 3.9
      fps: 15.4
      box_AP: 37.4
    - model: R-50-FPN
      lr_schedule: 1x
      mem_gb: 3.3
      fps: 18.5
      box_AP: 37.0
    - model: R-50-FPN
      lr_schedule: 1x
      mem_gb: 3.9
      fps: 17.5
      box_AP: 38.1
    - model: R-50-FPN
      lr_schedule: 2x
      mem_gb: 3.9
      box_AP: 38.6
    - model: R-101-FPN
      lr_schedule: 2x
      mem_gb: 5.8
      fps: 13.7
      box_AP: 40.5
    - model: R-101-FPN-DCN
      lr_schedule: 2x
      mem_gb: 5.9
      fps: 12.1
      box_AP: 42.9
    - model: X-101-FPN-DCN
      lr_schedule: 2x
      mem_gb: 7.1
      fps: 9.3
      box_AP: 44.2
  - repository: mmdetection
    file: configs/cascade_rcnn/README.md
    config: cascade_rcnn
    algorithm: Cascade R-CNN
    models:
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 4.2
      box_AP: 40.4
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.4
      fps: 16.1
      box_AP: 40.3
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 20e
      box_AP: 41.0
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 6.2
      box_AP: 42.3
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.4
      fps: 13.5
      box_AP: 42.0
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 20e
      box_AP: 42.5
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.6
      fps: 10.9
      box_AP: 43.7
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 7.6
      box_AP: 43.7
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.7
      box_AP: 44.7
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 10.7
      box_AP: 44.5
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.9
      box_AP: 41.2
      mask_AP: 36.0
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.0
      fps: 11.2
      box_AP: 41.2
      mask_AP: 35.9
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 20e
      box_AP: 41.9
      mask_AP: 36.5
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 7.8
      box_AP: 43.2
      mask_AP: 37.6
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.9
      fps: 9.8
      box_AP: 42.9
      mask_AP: 37.3
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 20e
      box_AP: 43.4
      mask_AP: 37.8
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.2
      fps: 8.6
      box_AP: 44.3
      mask_AP: 38.3
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 9.2
      box_AP: 45.0
      mask_AP: 39.0
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 12.2
      fps: 6.7
      box_AP: 45.3
      mask_AP: 39.2
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 12.2
      box_AP: 45.6
      mask_AP: 39.5
    - model: R-50-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 5.7
      box_AP: 44.0
      mask_AP: 38.1
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.9
      box_AP: 44.3
      mask_AP: 38.5
    - model: R-101-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 7.7
      box_AP: 45.4
      mask_AP: 39.5
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 7.8
      box_AP: 45.5
      mask_AP: 39.6
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 9.0
      box_AP: 46.3
      mask_AP: 40.1
    - model: X-101-32x8d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 12.1
      box_AP: 46.1
      mask_AP: 39.9
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 12.0
      box_AP: 46.6
      mask_AP: 40.3
  - repository: mmdetection
    file: configs/ld/README.md
    config: ld
    algorithm: LD
    models:
    - mini_batch_size: 6.0
      AP: 35.8
    - mini_batch_size: 6.0
      AP: 36.5
    - mini_batch_size: 6.0
      AP: 38.9
    - mini_batch_size: 6.0
      AP: 39.9
    - mini_batch_size: 6.0
      AP: 40.1
    - mini_batch_size: 6.0
      AP: 41.0
    - mini_batch_size: 6.0
      AP: 44.6
    - mini_batch_size: 6.0
      AP: 45.5
  - repository: mmdetection
    file: configs/yolact/README.md
    config: yolact
    algorithm: YOLACT
    models:
    - image_size: 550.0
      model: Resnet50-FPN
      fps: 42.5
      mAP: 29.0
    - image_size: 550.0
      model: Resnet50-FPN
      fps: 42.5
      mAP: 28.4
    - image_size: 550.0
      model: Resnet101-FPN
      fps: 33.5
      mAP: 30.4
  - repository: mmdetection
    file: configs/dab_detr/README.md
    config: dab_detr
    algorithm: DAB-DETR
    models:
    - model: DAB-DETR
      lr_schedule: 50e
      box_AP: 42.3
  - repository: mmdetection
    file: configs/dynamic_rcnn/README.md
    config: dynamic_rcnn
    algorithm: Dynamic R-CNN
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.8
      box_AP: 38.9
  - repository: mmdetection
    file: configs/resnest/README.md
    config: resnest
    algorithm: ResNeSt
    models:
    - model: S-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.8
      box_AP: 42.0
    - model: S-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.1
      box_AP: 44.5
    - model: S-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.5
      box_AP: 42.6
      mask_AP: 38.1
    - model: S-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.8
      box_AP: 45.2
      mask_AP: 40.2
    - model: S-50-FPN
      style: pytorch
      lr_schedule: 1x
      box_AP: 44.5
    - model: S-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.4
      box_AP: 46.8
    - model: S-50-FPN
      style: pytorch
      lr_schedule: 1x
      box_AP: 45.4
      mask_AP: 39.5
    - model: S-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.5
      box_AP: 47.7
      mask_AP: 41.4
  - repository: mmdetection
    file: configs/queryinst/README.md
    config: queryinst
    algorithm: QueryInst
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      number_of_proposals: 100.0
      box_AP: 42.0
      mask_AP: 37.5
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 100.0
      box_AP: 44.8
      mask_AP: 39.8
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 300.0
      box_AP: 47.5
      mask_AP: 41.7
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 100.0
      box_AP: 46.4
      mask_AP: 41.0
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 300.0
      box_AP: 49.0
      mask_AP: 42.9
  - repository: mmdetection
    file: configs/lad/README.md
    config: lad
    algorithm: LAD
    models:
    - AP: 40.4
    - AP: 42.6
    - AP: 41.4
    - AP: 43.2
  - repository: mmdetection
    file: configs/ddq/README.md
    config: ddq
    algorithm: DDQ
    models:
    - model: R-50
      lr_schedule: 12e
      box_AP: 51.4
    - model: R-50
      lr_schedule: 12e
      box_AP: 52.1
    - model: Swin-L
      lr_schedule: 30e
      box_AP: 58.7
  - repository: mmdetection
    file: configs/pvt/README.md
    config: pvt
    algorithm: PVT
    models:
    - model: PVT-Tiny
      lr_schedule: 12e
      mem_gb: 8.5
      box_AP: 36.6
    - model: PVT-Small
      lr_schedule: 12e
      mem_gb: 14.5
      box_AP: 40.4
    - model: PVT-Medium
      lr_schedule: 12e
      mem_gb: 20.9
      box_AP: 41.7
    - model: PVTv2-B0
      lr_schedule: 12e
      mem_gb: 7.4
      box_AP: 37.1
    - model: PVTv2-B1
      lr_schedule: 12e
      mem_gb: 9.5
      box_AP: 41.2
    - model: PVTv2-B2
      lr_schedule: 12e
      mem_gb: 16.2
      box_AP: 44.6
    - model: PVTv2-B3
      lr_schedule: 12e
      mem_gb: 23.0
      box_AP: 46.0
    - model: PVTv2-B4
      lr_schedule: 12e
      mem_gb: 17.0
      box_AP: 46.3
    - model: PVTv2-B5
      lr_schedule: 12e
      mem_gb: 18.7
      box_AP: 46.1
  - repository: mmdetection
    file: configs/empirical_attention/README.md
    config: empirical_attention
    algorithm: Empirical Attention
    models:
    - model: R-50
      attention_component: 1111.0
      lr_schedule: 1x
      mem_gb: 8.0
      fps: 13.8
      box_AP: 40.0
    - model: R-50
      attention_component: 10.0
      lr_schedule: 1x
      mem_gb: 4.2
      fps: 18.4
      box_AP: 39.1
    - model: R-50
      attention_component: 1111.0
      lr_schedule: 1x
      mem_gb: 8.0
      fps: 12.7
      box_AP: 42.1
    - model: R-50
      attention_component: 10.0
      lr_schedule: 1x
      mem_gb: 4.2
      fps: 17.1
      box_AP: 42.0
  - repository: mmdetection
    file: configs/glip/README.md
    config: glip
    algorithm: 'GLIP: Grounded Language-Image Pre-training'
    models:
    - model: GLIP-T (A)
      mAP: 42.9
    - model: GLIP-T (A)
      mAP: 52.9
    - model: GLIP-T (B)
      mAP: 44.9
    - model: GLIP-T (B)
      mAP: 53.8
    - model: GLIP-T (C)
      mAP: 46.7
    - model: GLIP-T (C)
      mAP: 55.1
    - model: GLIP-T
      mAP: 46.6
    - model: GLIP-T
      mAP: 55.2
    - model: GLIP-L
      mAP: 51.4
    - model: GLIP-L
      mAP: 59.4
    - model: GLIP-T (A)
    - model: GLIP-T (A)
      AP: 14.7
    - model: GLIP-T (B)
    - model: GLIP-T (B)
      AP: 13.9
    - model: GLIP-T (C)
      AP: 24.6
    - model: GLIP-T (C)
      AP: 18.2
    - model: GLIP-T
    - model: GLIP-T
      AP: 19.6
    - model: GLIP-L
      AP: 37.9
    - model: GLIP-L
      AP: 28.5
  - repository: mmdetection
    file: configs/masktrack_rcnn/README.md
    config: masktrack_rcnn
    algorithm: Video Instance Segmentation
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 12e
      mem_gb: 1.61
      AP: 30.2
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 12e
      mem_gb: 2.27
      AP: 32.2
    - model: X-101-FPN
      style: pytorch
      lr_schedule: 12e
      mem_gb: 3.69
      AP: 34.7
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 12e
      mem_gb: 1.61
      AP: 28.7
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 12e
      mem_gb: 2.27
      AP: 31.3
    - model: X-101-FPN
      style: pytorch
      lr_schedule: 12e
      mem_gb: 3.69
      AP: 33.5
  - repository: mmdetection
    file: configs/mask2former/README.md
    config: mask2former
    algorithm: Mask2Former
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 50e
      mem_gb: 13.9
      pq: 52.0
      box_AP: 44.5
      mask_AP: 41.8
    - model: R-101
      style: pytorch
      lr_schedule: 50e
      mem_gb: 16.1
      pq: 52.4
      box_AP: 45.3
      mask_AP: 42.4
    - model: Swin-T
      style: '-'
      lr_schedule: 50e
      mem_gb: 15.9
      pq: 53.4
      box_AP: 46.3
      mask_AP: 43.4
    - model: Swin-S
      style: '-'
      lr_schedule: 50e
      mem_gb: 19.1
      pq: 54.5
      box_AP: 47.8
      mask_AP: 44.5
    - model: Swin-B
      style: '-'
      lr_schedule: 50e
      mem_gb: 26.0
      pq: 55.1
      box_AP: 48.2
      mask_AP: 44.9
    - model: Swin-B
      style: '-'
      lr_schedule: 50e
      mem_gb: 25.8
      pq: 56.3
      box_AP: 50.0
      mask_AP: 46.3
    - model: Swin-L
      style: '-'
      lr_schedule: 100e
      mem_gb: 21.1
      pq: 57.6
      box_AP: 52.2
      mask_AP: 48.5
    - model: R-50
      style: pytorch
      lr_schedule: 50e
      mem_gb: 13.7
      box_AP: 45.7
      mask_AP: 42.9
    - model: R-101
      style: pytorch
      lr_schedule: 50e
      mem_gb: 15.5
      box_AP: 46.7
      mask_AP: 44.0
    - model: Swin-T
      style: '-'
      lr_schedule: 50e
      mem_gb: 15.3
      box_AP: 47.7
      mask_AP: 44.7
    - model: Swin-S
      style: '-'
      lr_schedule: 50e
      mem_gb: 18.8
      box_AP: 49.3
      mask_AP: 46.1
  - repository: mmdetection
    file: configs/grounding_dino/README.md
    config: grounding_dino
    algorithm: 'Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection'
    models:
    - model: Swin-T
      style: Zero-shot
      mAP: 48.4
    - model: Swin-T
      style: Finetune
      mAP: 57.2
    - model: Swin-B
      style: Zero-shot
      mAP: 56.7
    - model: Swin-B
      style: Finetune
      mAP: 59.7
    - model: R50
      style: Scratch
      mAP: 48.1
    - model: Grounding DINO-T
      AP: 20.1
    - model: Grounding DINO-B
      AP: 26.7
    - model: RefCOCO val @1,5,10
      AP: 84.61
    - model: RefCOCO testA @1,5,10
      AP: 88.65
    - model: RefCOCO testB @1,5,10
      AP: 80.51
    - model: RefCOCO+ val @1,5,10
      AP: 73.67
    - model: RefCOCO+ testA @1,5,10
      AP: 82.19
    - model: RefCOCO+ testB @1,5,10
      AP: 64.1
    - model: RefCOCOg val @1,5,10
      AP: 78.33
    - model: RefCOCOg test @1,5,10
      AP: 78.11
    - model: gRefCOCO val Pr@(F1=1, IoU≥0.5),N-acc
      AP: 46.18
    - model: gRefCOCO testA Pr@(F1=1, IoU≥0.5),N-acc
      AP: 38.6
    - model: gRefCOCO testB Pr@(F1=1, IoU≥0.5),N-acc
      AP: 35.87
    - model: FULL/short/middle/long/very long
      AP: 20.2
    - model: FULL/short/middle/long/very long
      AP: 25.0
    - model: PRES/short/middle/long/very long
      AP: 20.7
    - model: PRES/short/middle/long/very long
      AP: 23.7
    - model: ABS/short/middle/long/very long
      AP: 18.6
    - model: ABS/short/middle/long/very long
      AP: 28.8
  - repository: mmdetection
    file: configs/pafpn/README.md
    config: pafpn
    algorithm: PAFPN
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.0
      fps: 17.2
      box_AP: 37.5
  - repository: mmdetection
    file: configs/boxinst/README.md
    config: boxinst
    algorithm: BoxInst
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 39.6
      mask_AP: 31.1
    - model: R-101
      style: pytorch
      lr_schedule: 1x
      box_AP: 41.8
      mask_AP: 32.7
  - repository: mmdetection
    file: configs/pascal_voc/README.md
    config: pascal_voc
    algorithm: Pascal VOC
    models:
    - model: R-50
      style: caffe
      lr_schedule: 18k
      box_AP: 80.9
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 2.6
      box_AP: 80.4
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 2.1
      box_AP: 77.3
    - model: VGG16
      style: '-'
      lr_schedule: 120e
      box_AP: 76.5
    - model: VGG16
      style: '-'
      lr_schedule: 120e
      box_AP: 79.5
  - repository: mmdetection
    file: configs/centripetalnet/README.md
    config: centripetalnet
    algorithm: CentripetalNet
    models:
    - model: HourglassNet-104
      mem_gb: 16.7
      fps: 3.7
      box_AP: 44.8
  - repository: mmdetection
    file: configs/scnet/README.md
    config: scnet
    algorithm: SCNet
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.0
      fps: 6.2
      box_AP: 44.8
      mask_AP: 40.9
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 7.0
      fps: 6.2
      box_AP: 45.8
      mask_AP: 41.5
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 8.9
      fps: 5.8
      box_AP: 47.3
      mask_AP: 42.7
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 13.2
      fps: 4.9
      box_AP: 48.9
      mask_AP: 44.0
  - repository: mmdetection
    file: configs/efficientnet/README.md
    config: efficientnet
    algorithm: EfficientNet
    models:
    - model: Efficientnet-b3
      style: pytorch
      lr_schedule: 1x
      box_AP: 40.5
  - repository: mmdetection
    file: configs/sabl/README.md
    config: sabl
    algorithm: SABL
    models:
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 39.9
    - model: R-101-FPN
      lr_schedule: 1x
      box_AP: 41.7
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 41.6
    - model: R-101-FPN
      lr_schedule: 1x
      box_AP: 43.0
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 37.7
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 38.8
    - model: R-101-FPN
      lr_schedule: 1x
      box_AP: 39.7
    - model: R-101-FPN
      lr_schedule: 1x
      box_AP: 40.5
    - model: R-101-FPN
      lr_schedule: 2x
      box_AP: 42.9
    - model: R-101-FPN
      lr_schedule: 2x
      box_AP: 43.6
  - repository: mmdetection
    file: configs/fsaf/README.md
    config: fsaf
    algorithm: FSAF
    models:
    - model: R-50
      lr_schedule: 1x
      train_mem_gb: 3.15
      fps: 12.3
      box_AP: 36.0
    - model: R-50
      lr_schedule: 1x
      train_mem_gb: 3.15
      fps: 13.0
      box_AP: 37.4
    - model: R-101
      lr_schedule: 1x
      train_mem_gb: 5.08
      fps: 10.8
      box_AP: 39.3
    - model: X-101
      lr_schedule: 1x
      train_mem_gb: 9.38
      fps: 5.6
      box_AP: 42.4
  - repository: mmdetection
    file: configs/vfnet/README.md
    config: vfnet
    algorithm: VarifocalNet
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 41.6
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      box_AP: 44.8
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      box_AP: 48.0
    - model: R-101
      style: pytorch
      lr_schedule: 1x
      box_AP: 43.6
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      box_AP: 46.7
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      box_AP: 49.2
    - model: X-101-32x4d
      style: pytorch
      lr_schedule: 2x
      box_AP: 50.0
    - model: X-101-64x4d
      style: pytorch
      lr_schedule: 2x
      box_AP: 50.8
  - repository: mmdetection
    file: configs/nas_fpn/README.md
    config: nas_fpn
    algorithm: NAS-FPN
    models:
    - model: R-50-FPN
      lr_schedule: 50e
      mem_gb: 12.9
      fps: 22.9
      box_AP: 37.9
    - model: R-50-NASFPN
      lr_schedule: 50e
      mem_gb: 13.2
      fps: 23.0
      box_AP: 40.5
  - repository: mmdetection
    file: configs/gcnet/README.md
    config: gcnet
    algorithm: GCNet
    models:
    - model: Mask
      lr_schedule: 1x
      mem_gb: 5.0
      box_AP: 39.7
      mask_AP: 35.9
    - model: Mask
      lr_schedule: 1x
      mem_gb: 5.1
      fps: 15.0
      box_AP: 39.9
      mask_AP: 36.0
    - model: Mask
      lr_schedule: 1x
      mem_gb: 7.6
      fps: 11.4
      box_AP: 41.3
      mask_AP: 37.2
    - model: Mask
      lr_schedule: 1x
      mem_gb: 7.8
      fps: 11.6
      box_AP: 42.2
      mask_AP: 37.8
    - model: Mask
      lr_schedule: 1x
      mem_gb: 4.4
      fps: 16.6
      box_AP: 38.4
      mask_AP: 34.6
    - model: Mask
      lr_schedule: 1x
      mem_gb: 5.0
      fps: 15.5
      box_AP: 40.4
      mask_AP: 36.2
    - model: Mask
      lr_schedule: 1x
      mem_gb: 5.1
      fps: 15.1
      box_AP: 40.7
      mask_AP: 36.5
    - model: Mask
      lr_schedule: 1x
      mem_gb: 6.4
      fps: 13.3
      box_AP: 40.5
      mask_AP: 36.3
    - model: Mask
      lr_schedule: 1x
      mem_gb: 7.6
      fps: 12.0
      box_AP: 42.2
      mask_AP: 37.8
    - model: Mask
      lr_schedule: 1x
      mem_gb: 7.8
      fps: 11.8
      box_AP: 42.2
      mask_AP: 37.8
    - model: Mask
      lr_schedule: 1x
      mem_gb: 7.6
      fps: 11.3
      box_AP: 42.4
      mask_AP: 37.7
    - model: Mask
      lr_schedule: 1x
      mem_gb: 8.8
      fps: 9.8
      box_AP: 43.5
      mask_AP: 38.6
    - model: Mask
      lr_schedule: 1x
      mem_gb: 9.0
      fps: 9.7
      box_AP: 43.9
      mask_AP: 39.0
    - model: Cascade Mask
      lr_schedule: 1x
      mem_gb: 9.2
      fps: 8.4
      box_AP: 44.7
      mask_AP: 38.6
    - model: Cascade Mask
      lr_schedule: 1x
      mem_gb: 10.3
      fps: 7.7
      box_AP: 46.2
      mask_AP: 39.7
    - model: Cascade Mask
      lr_schedule: 1x
      mem_gb: 10.6
      box_AP: 46.4
      mask_AP: 40.1
    - model: DCN Cascade Mask
      lr_schedule: 1x
      box_AP: 47.5
      mask_AP: 40.9
    - model: DCN Cascade Mask
      lr_schedule: 1x
      box_AP: 48.0
      mask_AP: 41.3
    - model: DCN Cascade Mask
      lr_schedule: 1x
      box_AP: 47.9
      mask_AP: 41.1
  - repository: mmdetection
    file: configs/point_rend/README.md
    config: point_rend
    algorithm: PointRend
    models:
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 4.6
      box_AP: 38.4
      mask_AP: 36.3
    - model: R-50-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 4.6
      box_AP: 41.0
      mask_AP: 38.0
  - repository: mmdetection
    file: configs/dino/README.md
    config: dino
    algorithm: DINO
    models:
    - model: DINO-4scale
      lr_schedule: 12e
      box_AP: 49.0
    - model: DINO-4scale
      lr_schedule: 12e
      box_AP: 50.1
    - model: DINO-5scale
      lr_schedule: 12e
      box_AP: 57.2
    - model: DINO-5scale
      lr_schedule: 36e
      box_AP: 58.4
  - repository: mmdetection
    file: configs/mask_rcnn/README.md
    config: mask_rcnn
    algorithm: Mask R-CNN
    models:
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 4.3
      box_AP: 38.0
      mask_AP: 34.4
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.4
      fps: 16.1
      box_AP: 38.2
      mask_AP: 34.7
    - model: R-50-FPN (FP16)
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.6
      fps: 24.1
      box_AP: 38.1
      mask_AP: 34.7
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 39.2
      mask_AP: 35.4
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      box_AP: 40.4
      mask_AP: 36.4
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.4
      fps: 13.5
      box_AP: 40.0
      mask_AP: 36.1
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 40.8
      mask_AP: 36.6
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.6
      fps: 11.3
      box_AP: 41.9
      mask_AP: 37.5
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 42.2
      mask_AP: 37.8
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.7
      fps: 8.0
      box_AP: 42.8
      mask_AP: 38.4
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 42.7
      mask_AP: 38.1
    - model: X-101-32x8d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.6
      box_AP: 42.8
      mask_AP: 38.3
    - model: R-50-FPN
      style: caffe
      lr_schedule: 2x
      mem_gb: 4.3
      box_AP: 40.3
      mask_AP: 36.5
    - model: R-50-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 4.3
      box_AP: 40.8
      mask_AP: 37.0
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 4.1
      box_AP: 40.9
      mask_AP: 37.1
    - model: R-101-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 5.9
      box_AP: 42.9
      mask_AP: 38.5
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 6.1
      box_AP: 42.7
      mask_AP: 38.5
    - model: x101-32x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 7.3
      box_AP: 43.6
      mask_AP: 39.0
    - model: X-101-32x8d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.4
      box_AP: 43.4
      mask_AP: 39.0
    - model: X-101-32x8d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 10.3
      box_AP: 44.3
      mask_AP: 39.5
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 10.4
      box_AP: 44.5
      mask_AP: 39.7
  - repository: mmdetection
    file: configs/openimages/README.md
    config: openimages
    algorithm: Open Images Dataset
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.7
      box_AP: 51.6
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.7
      box_AP: 60.0
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.7
      box_AP: 54.9
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.1
      box_AP: 65.0
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.6
      box_AP: 61.5
    - model: VGG16
      style: pytorch
      lr_schedule: 36e
      mem_gb: 10.8
      box_AP: 35.4
    - model: Faster R-CNN r50 (Challenge 2019)
      box_AP: 62.19
    - model: Faster R-CNN r50 (Challenge 2019)
      box_AP: 54.87
    - model: Faster R-CNN r50 (Challenge 2019)
      box_AP: 71.77
    - model: Faster R-CNN r50 (Challenge 2019)
      box_AP: 64.98
  - repository: mmdetection
    file: configs/carafe/README.md
    config: carafe
    algorithm: CARAFE
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      test_proposal_num: 1000.0
      fps: 16.5
      box_AP: 38.6
      mask_AP: 38.6
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      test_proposal_num: 1000.0
      fps: 14.0
      box_AP: 39.3
      mask_AP: 35.8
  - repository: mmdetection
    file: configs/deformable_detr/README.md
    config: deformable_detr
    algorithm: Deformable DETR
    models:
    - model: Deformable DETR
      lr_schedule: 50e
      box_AP: 44.3
    - model: + iterative bounding box refinement
      lr_schedule: 50e
      box_AP: 46.2
    - model: ++ two-stage Deformable DETR
      lr_schedule: 50e
      box_AP: 47.0
  - repository: mmdetection
    file: configs/scratch/README.md
    config: scratch
    algorithm: Scratch
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 6x
      box_AP: 40.7
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 6x
      box_AP: 41.2
      mask_AP: 37.4
  - repository: mmdetection
    file: configs/ghm/README.md
    config: ghm
    algorithm: GHM
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.0
      fps: 3.3
      box_AP: 37.0
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.0
      fps: 4.4
      box_AP: 39.1
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.2
      fps: 5.1
      box_AP: 40.7
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.3
      fps: 5.2
      box_AP: 41.4
  - repository: mmdetection
    file: configs/sparse_rcnn/README.md
    config: sparse_rcnn
    algorithm: Sparse R-CNN
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      number_of_proposals: 100.0
      box_AP: 37.9
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 100.0
      box_AP: 42.8
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 300.0
      box_AP: 45.0
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 100.0
      box_AP: 44.2
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      number_of_proposals: 300.0
      box_AP: 46.2
  - repository: mmdetection
    file: configs/solo/README.md
    config: solo
    algorithm: SOLO
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.0
      fps: 14.0
      mask_AP: 33.1
    - model: R-50
      style: pytorch
      lr_schedule: 3x
      mem_gb: 7.4
      fps: 14.0
      mask_AP: 35.9
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.8
      fps: 12.5
      mask_AP: 33.9
    - model: R-50
      style: pytorch
      lr_schedule: 3x
      mem_gb: 7.9
      fps: 12.5
      mask_AP: 36.7
    - model: R-50
      style: pytorch
      lr_schedule: 3x
      mem_gb: 2.2
      fps: 31.2
      mask_AP: 32.9
  - repository: mmdetection
    file: configs/v3det/README.md
    config: v3det
    algorithm: '<img src="v3det_icon.jpg" height="25"> V3Det: Vast Vocabulary Visual Detection Dataset'
    models:
    - model: Faster R-CNN
      lr_schedule: 2x
      box_AP: 25.4
    - model: Cascade R-CNN
      lr_schedule: 2x
      box_AP: 31.6
    - model: FCOS
      lr_schedule: 2x
      box_AP: 9.4
    - model: Deformable-DETR
      lr_schedule: 50e
      box_AP: 34.4
    - model: DINO
      lr_schedule: 36e
      box_AP: 33.5
    - model: Faster R-CNN
      lr_schedule: 2x
      box_AP: 37.6
    - model: Cascade R-CNN
      lr_schedule: 2x
      box_AP: 42.5
    - model: FCOS
      lr_schedule: 2x
      box_AP: 21.0
    - model: Deformable-DETR
      lr_schedule: 50e
      box_AP: 42.5
    - model: DINO
      lr_schedule: 36e
      box_AP: 42.0
  - repository: mmdetection
    file: configs/dsdl/README.md
    config: dsdl
    algorithm: 'DSDL: Standard Description Language for DataSet'
    models:
    - model: model
      box_AP: 80.3
    - model: model
      box_AP: 37.4
    - model: model
      box_AP: 19.8
    - model: model
      box_AP: 59.9
    - model: model
      box_AP: 38.1
      mask_AP: 34.7
  - repository: mmdetection
    file: configs/cascade_rpn/README.md
    config: cascade_rpn
    algorithm: Cascade RPN
    models:
    - model: R-50-FPN
      style: caffe
      box_AP: 39.9
    - model: R-50-FPN
      style: caffe
      box_AP: 40.4
  - repository: mmdetection
    file: configs/fcos/README.md
    config: fcos
    algorithm: FCOS
    models:
    - model: R-50
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.6
      fps: 22.7
      box_AP: 36.6
    - model: R-50
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.7
      box_AP: 38.7
    - model: R-50
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.8
      box_AP: 42.3
    - model: R-101
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.5
      fps: 17.3
      box_AP: 39.1
    - model: R-50
      style: caffe
      lr_schedule: 2x
      mem_gb: 2.6
      fps: 22.9
      box_AP: 38.5
    - model: R-101
      style: caffe
      lr_schedule: 2x
      mem_gb: 5.5
      fps: 17.3
      box_AP: 40.8
    - model: X-101
      style: pytorch
      lr_schedule: 2x
      mem_gb: 10.0
      fps: 9.7
      box_AP: 42.6
  - repository: mmdetection
    file: configs/double_heads/README.md
    config: double_heads
    algorithm: Double Heads
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.8
      fps: 9.5
      box_AP: 40.0
  - repository: mmdetection
    file: configs/dcn/README.md
    config: dcn
    algorithm: DCN
    models:
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.0
      fps: 17.8
      box_AP: 41.3
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.0
      fps: 17.2
      box_AP: 38.9
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.0
      fps: 12.5
      box_AP: 42.7
    - model: Faster
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.3
      fps: 10.0
      box_AP: 44.5
    - model: Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.5
      fps: 15.4
      box_AP: 41.8
      mask_AP: 37.4
    - model: Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.5
      fps: 11.7
      box_AP: 43.5
      mask_AP: 38.9
    - model: Cascade
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.5
      fps: 14.6
      box_AP: 43.8
    - model: Cascade
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.4
      fps: 11.0
      box_AP: 45.0
    - model: Cascade Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.0
      fps: 10.0
      box_AP: 44.4
      mask_AP: 38.6
    - model: Cascade Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.0
      fps: 8.6
      box_AP: 45.8
      mask_AP: 39.7
    - model: Cascade Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.2
      box_AP: 47.3
      mask_AP: 41.1
    - model: Mask
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.0
      box_AP: 41.9
      mask_AP: 37.5
  - repository: mmdetection
    file: configs/retinanet/README.md
    config: retinanet
    algorithm: RetinaNet
    models:
    - model: R-18-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 1.7
      box_AP: 31.7
    - model: R-18-FPN
      style: pytorch
      lr_schedule: 1x(1 x 8 BS)
      mem_gb: 5.0
      box_AP: 31.7
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.5
      fps: 18.6
      box_AP: 36.3
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.8
      fps: 19.0
      box_AP: 36.5
    - model: R-50-FPN (FP16)
      style: pytorch
      lr_schedule: 1x
      mem_gb: 2.8
      fps: 31.6
      box_AP: 36.4
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 37.4
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.5
      fps: 14.7
      box_AP: 38.5
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.7
      fps: 15.0
      box_AP: 38.5
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 38.9
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.0
      fps: 12.1
      box_AP: 39.9
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 40.1
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.0
      fps: 8.7
      box_AP: 41.0
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 40.8
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 3.5
      box_AP: 39.5
    - model: R-101-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 5.4
      box_AP: 40.7
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.4
      box_AP: 41.0
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 9.8
      box_AP: 41.6
  - repository: mmdetection
    file: configs/rtmdet/README.md
    config: rtmdet
    algorithm: 'RTMDet: An Empirical Study of Designing Real-Time Object Detectors'
    models:
    - model: RTMDet-tiny
      size: 640.0
      box_AP: 41.1
      params_m: 4.8
      flops_g: 8.1
      trt_fp16_latency_ms_br_rtx3090: 0.98
      trt_fp16_latency_ms_br_t4: 2.34
    - model: RTMDet-s
      size: 640.0
      box_AP: 44.6
      params_m: 8.89
      flops_g: 14.8
      trt_fp16_latency_ms_br_rtx3090: 1.22
      trt_fp16_latency_ms_br_t4: 2.96
    - model: RTMDet-m
      size: 640.0
      box_AP: 49.4
      params_m: 24.71
      flops_g: 39.27
      trt_fp16_latency_ms_br_rtx3090: 1.62
      trt_fp16_latency_ms_br_t4: 6.41
    - model: RTMDet-l
      size: 640.0
      box_AP: 51.5
      params_m: 52.3
      flops_g: 80.23
      trt_fp16_latency_ms_br_rtx3090: 2.44
      trt_fp16_latency_ms_br_t4: 10.32
    - model: RTMDet-x
      size: 640.0
      box_AP: 52.8
      params_m: 94.86
      flops_g: 141.67
      trt_fp16_latency_ms_br_rtx3090: 3.1
      trt_fp16_latency_ms_br_t4: 18.8
    - model: RTMDet-x-P6
      size: 1280.0
      box_AP: 54.9
    - model: RTMDet-l-ConvNeXt-B
      size: 640.0
      box_AP: 53.1
    - model: RTMDet-l-Swin-B
      size: 640.0
      box_AP: 52.4
    - model: RTMDet-l-Swin-B-P6
      size: 1280.0
      box_AP: 56.4
    - model: RTMDet-Ins-tiny
      size: 640.0
      box_AP: 40.5
      mask_AP: 35.4
      params_m: 5.6
      flops_g: 11.8
      trt_fp16_latency_ms: 1.7
    - model: RTMDet-Ins-s
      size: 640.0
      box_AP: 44.0
      mask_AP: 38.7
      params_m: 10.18
      flops_g: 21.5
      trt_fp16_latency_ms: 1.93
    - model: RTMDet-Ins-m
      size: 640.0
      box_AP: 48.8
      mask_AP: 42.1
      params_m: 27.58
      flops_g: 54.13
      trt_fp16_latency_ms: 2.69
    - model: RTMDet-Ins-l
      size: 640.0
      box_AP: 51.2
      mask_AP: 43.7
      params_m: 57.37
      flops_g: 106.56
      trt_fp16_latency_ms: 3.68
    - model: RTMDet-Ins-x
      size: 640.0
      box_AP: 52.4
      mask_AP: 44.6
      params_m: 102.7
      flops_g: 182.7
      trt_fp16_latency_ms: 5.31
    - model: RTMDet-tiny
      mAP: 50.64
      params_m: 4.88
      flops_g: 20.45
      trt_fp16_latency_ms: 4.4
    - model: RTMDet-tiny
      mAP: 58.87
      params_m: 4.88
      flops_g: 20.45
      trt_fp16_latency_ms: 4.4
    - model: RTMDet-s
      mAP: 50.59
      params_m: 8.86
      flops_g: 37.62
      trt_fp16_latency_ms: 4.86
    - model: RTMDet-s
      mAP: 60.07
      params_m: 8.86
      flops_g: 37.62
      trt_fp16_latency_ms: 4.86
    - model: RTMDet-m
      mAP: 54.47
      params_m: 24.67
      flops_g: 99.76
      trt_fp16_latency_ms: 7.82
    - model: RTMDet-m
      mAP: 61.26
      params_m: 24.67
      flops_g: 99.76
      trt_fp16_latency_ms: 7.82
    - model: RTMDet-l
      mAP: 55.21
      params_m: 52.27
      flops_g: 204.21
      trt_fp16_latency_ms: 10.82
    - model: RTMDet-l
      mAP: 61.47
      params_m: 52.27
      flops_g: 204.21
      trt_fp16_latency_ms: 10.82
    - model: RTMDet-l
      mAP: 63.45
      params_m: 52.27
      flops_g: 204.21
      trt_fp16_latency_ms: 10.82
  - repository: mmdetection
    file: configs/grid_rcnn/README.md
    config: grid_rcnn
    algorithm: Grid R-CNN
    models:
    - model: R-50
      lr_schedule: 2x
      mem_gb: 5.1
      fps: 15.0
      box_AP: 40.4
    - model: R-101
      lr_schedule: 2x
      mem_gb: 7.0
      fps: 12.6
      box_AP: 41.5
    - model: X-101-32x4d
      lr_schedule: 2x
      mem_gb: 8.3
      fps: 10.8
      box_AP: 42.9
    - model: X-101-64x4d
      lr_schedule: 2x
      mem_gb: 11.3
      fps: 7.7
      box_AP: 43.0
  - repository: mmdetection
    file: configs/dyhead/README.md
    config: dyhead
    algorithm: DyHead
    models:
    - model: R-50
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.4
      fps: 13.2
      box_AP: 42.5
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.9
      fps: 13.7
      box_AP: 43.3
    - model: Swin-L
      style: caffe
      lr_schedule: 2x
      box_AP: 56.2
  - repository: mmdetection
    file: configs/objects365/README.md
    config: objects365
    algorithm: Objects365 Dataset
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 19.6
    - model: R-50
      style: pytorch
      lr_schedule: 1350K
      box_AP: 22.3
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 14.8
    - model: R-50
      style: pytorch
      lr_schedule: 1350K
      box_AP: 18.0
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 19.8
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 16.7
  - repository: mmdetection
    file: configs/gn+ws/README.md
    config: gn+ws
    algorithm: GN + WS
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.9
      fps: 11.7
      box_AP: 39.7
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.9
      fps: 9.0
      box_AP: 41.7
    - model: X-50-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.0
      fps: 10.3
      box_AP: 40.7
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.8
      fps: 7.6
      box_AP: 42.1
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 7.3
      fps: 10.5
      box_AP: 40.6
      mask_AP: 36.6
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 10.3
      fps: 8.6
      box_AP: 42.0
      mask_AP: 37.7
    - model: X-50-32x4d-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 8.4
      fps: 9.3
      box_AP: 41.1
      mask_AP: 37.0
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 12.2
      fps: 7.1
      box_AP: 42.1
      mask_AP: 37.9
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 20-23-24e
      mem_gb: 7.3
      box_AP: 41.1
      mask_AP: 37.1
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 20-23-24e
      mem_gb: 10.3
      box_AP: 43.1
      mask_AP: 38.6
    - model: X-50-32x4d-FPN
      style: pytorch
      lr_schedule: 20-23-24e
      mem_gb: 8.4
      box_AP: 42.1
      mask_AP: 38.0
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 20-23-24e
      mem_gb: 12.2
      box_AP: 42.7
      mask_AP: 38.5
  - repository: mmdetection
    file: configs/yolof/README.md
    config: yolof
    algorithm: YOLOF
    models:
    - model: R-50-C5
      style: caffe
      lr_schedule: 1x
      mem_gb: 8.3
      box_AP: 37.5
  - repository: mmdetection
    file: configs/pisa/README.md
    config: pisa
    algorithm: PISA
    models:
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 36.4
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 38.4
    - model: X101-32x4d-FPN
      lr_schedule: 1x
      box_AP: 40.1
    - model: X101-32x4d-FPN
      lr_schedule: 1x
      box_AP: 41.9
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 37.3
      mask_AP: 34.2
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 39.1
      mask_AP: 35.2
    - model: X101-32x4d-FPN
      lr_schedule: 1x
      box_AP: 41.1
      mask_AP: 37.1
    - model: X101-32x4d-FPN
      lr_schedule: 1x
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 35.6
    - model: R-50-FPN
      lr_schedule: 1x
      box_AP: 36.9
    - model: X101-32x4d-FPN
      lr_schedule: 1x
      box_AP: 39.0
    - model: X101-32x4d-FPN
      lr_schedule: 1x
      box_AP: 40.7
    - model: VGG16
      lr_schedule: 1x
      box_AP: 25.6
    - model: VGG16
      lr_schedule: 1x
      box_AP: 27.6
    - model: VGG16
      lr_schedule: 1x
      box_AP: 29.3
    - model: VGG16
      lr_schedule: 1x
      box_AP: 31.8
  - repository: mmdetection
    file: configs/ddod/README.md
    config: ddod
    algorithm: DDOD
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.4
      box_AP: 41.7
  - repository: mmdetection
    file: configs/res2net/README.md
    config: res2net
    algorithm: Res2Net
    models:
    - model: R2-101-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 7.4
      box_AP: 43.0
    - model: R2-101-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 7.9
      box_AP: 43.6
      mask_AP: 38.7
    - model: R2-101-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 7.8
      box_AP: 45.7
    - model: R2-101-FPN
      style: pytorch
      lr_schedule: 20e
      mem_gb: 9.5
      box_AP: 46.4
      mask_AP: 40.0
    - model: R2-101-FPN
      style: pytorch
      lr_schedule: 20e
      box_AP: 47.5
      mask_AP: 41.6
  - repository: mmdetection
    file: configs/faster_rcnn/README.md
    config: faster_rcnn
    algorithm: Faster R-CNN
    models:
    - model: R-50-C4
      style: caffe
      lr_schedule: 1x
      box_AP: 35.6
    - model: R-50-DC5
      style: caffe
      lr_schedule: 1x
      box_AP: 37.2
    - model: R-50-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 3.8
      box_AP: 37.8
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.0
      fps: 21.4
      box_AP: 37.4
    - model: R-50-FPN (FP16)
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.4
      fps: 28.8
      box_AP: 37.5
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 38.4
    - model: R-101-FPN
      style: caffe
      lr_schedule: 1x
      mem_gb: 5.7
      box_AP: 39.8
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.0
      fps: 15.6
      box_AP: 39.4
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 39.8
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.2
      fps: 13.8
      box_AP: 41.2
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 41.2
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 10.3
      fps: 9.4
      box_AP: 42.1
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 41.6
    - model: R-50-FPN
      mem_gb: 4.0
      fps: 21.4
      box_AP: 37.4
    - model: R-50-FPN
      box_AP: 37.9
    - model: R-50-FPN
      box_AP: 37.6
    - model: R-50-FPN
      box_AP: 37.4
    - model: R-50-C4
      style: caffe
      lr_schedule: 1x
      box_AP: 35.9
    - model: R-50-DC5
      style: caffe
      lr_schedule: 1x
      box_AP: 37.4
    - model: R-50-DC5
      style: caffe
      lr_schedule: 3x
      box_AP: 38.7
    - model: R-50-FPN
      style: caffe
      lr_schedule: 2x
      mem_gb: 3.7
      box_AP: 39.7
    - model: R-50-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 3.7
      box_AP: 39.9
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 3.9
      box_AP: 40.3
    - model: R-101-FPN
      style: caffe
      lr_schedule: 3x
      mem_gb: 5.6
      box_AP: 42.0
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.8
      box_AP: 41.8
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 7.0
      box_AP: 42.5
    - model: X-101-32x8d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 10.1
      box_AP: 42.4
    - model: X-101-64x4d-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 10.0
      box_AP: 43.1
    - model: R-50-FPN-Caffe-3x
      style: caffe
      mem_gb: 3.7
      box_AP: 55.8
    - model: R-50-FPN-Caffe-3x
      style: caffe
      mem_gb: 3.7
      box_AP: 44.1
    - model: R-50-TNR
      style: pytorch
      lr_schedule: 1x
      box_AP: 40.2
  - repository: mmdetection
    file: configs/albu_example/README.md
    config: albu_example
    algorithm: Albu Example
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.4
      fps: 16.6
      box_AP: 38.0
      mask_AP: 34.5
  - repository: mmdetection
    file: configs/solov2/README.md
    config: solov2
    algorithm: SOLOv2
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.1
      mask_AP: 34.8
    - model: R-50
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.1
      mask_AP: 37.5
    - model: R-101
      style: pytorch
      lr_schedule: 3x
      mem_gb: 6.9
      mask_AP: 39.1
    - model: R-101(DCN)
      style: pytorch
      lr_schedule: 3x
      mem_gb: 7.1
      mask_AP: 41.2
    - model: X-101(DCN)
      style: pytorch
      lr_schedule: 3x
      mem_gb: 11.3
      mask_AP: 42.4
    - model: R-18
      style: pytorch
      lr_schedule: 3x
      mem_gb: 9.1
      mask_AP: 29.7
    - model: R-34
      style: pytorch
      lr_schedule: 3x
      mem_gb: 9.3
      mask_AP: 31.9
    - model: R-50
      style: pytorch
      lr_schedule: 3x
      mem_gb: 9.9
      mask_AP: 33.7
  - repository: mmdetection
    file: configs/resnet_strikes_back/README.md
    config: resnet_strikes_back
    algorithm: ResNet strikes back
    models:
    - model: R-50 rsb
      lr_schedule: 1x
      mem_gb: 3.9
      box_AP: 40.8
    - model: R-50 rsb
      lr_schedule: 1x
      mem_gb: 4.5
      box_AP: 41.2
      mask_AP: 38.2
    - model: R-50 rsb
      lr_schedule: 1x
      mem_gb: 6.2
      box_AP: 44.8
      mask_AP: 39.9
    - model: R-50 rsb
      lr_schedule: 1x
      mem_gb: 3.8
      box_AP: 39.0
  - repository: mmdetection
    file: configs/mm_grounding_dino/README.md
    config: mm_grounding_dino
    algorithm: MM Grounding DINO
    models:
    - model: Swin-T
      style: Zero-shot
      mAP: 46.7
    - model: Swin-T
      style: Zero-shot
      mAP: 48.1
    - model: Swin-T
      style: Zero-shot
      mAP: 48.4
    - model: Swin-T
      style: Zero-shot
      mAP: 48.5
    - model: Swin-T
      style: Zero-shot
      mAP: 50.4
    - model: Swin-T
      style: Zero-shot
      mAP: 50.5
    - model: Swin-T
      style: Zero-shot
      mAP: 50.6
    - model: Swin-T
      style: Zero-shot
      mAP: 50.4
    - model: Swin-B
      style: Zero-shot
      mAP: 52.5
    - model: Swin-B
      style: '-'
      mAP: 59.5
    - model: Swin-L
      style: Zero-shot
      mAP: 53.0
    - model: Swin-L
      style: '-'
      mAP: 60.3
    - model: GDINO-T
      AP: 20.1
    - model: MM-GDINO-T
      AP: 27.0
    - model: MM-GDINO-T
      AP: 27.1
    - model: MM-GDINO-T
      AP: 30.6
    - model: MM-GDINO-T
      AP: 31.9
    - model: AerialMaritimeDrone
      AP: 0.173
      mm_gdino_t_br_o365_goldg: 0.133
      mm_gdino_t_br_o365_goldg_grit: 0.155
      mm_gdino_t_br_o365_goldg_v3det: 0.177
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.151
    - model: Aquarium
      AP: 0.195
      mm_gdino_t_br_o365_goldg: 0.252
      mm_gdino_t_br_o365_goldg_grit: 0.261
      mm_gdino_t_br_o365_goldg_v3det: 0.266
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.283
    - model: CottontailRabbits
      AP: 0.799
      mm_gdino_t_br_o365_goldg: 0.771
      mm_gdino_t_br_o365_goldg_grit: 0.81
      mm_gdino_t_br_o365_goldg_v3det: 0.778
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.786
    - model: EgoHands
      AP: 0.608
      mm_gdino_t_br_o365_goldg: 0.499
      mm_gdino_t_br_o365_goldg_grit: 0.537
      mm_gdino_t_br_o365_goldg_v3det: 0.506
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.519
    - model: NorthAmericaMushrooms
      AP: 0.507
      mm_gdino_t_br_o365_goldg: 0.331
      mm_gdino_t_br_o365_goldg_grit: 0.462
      mm_gdino_t_br_o365_goldg_v3det: 0.669
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.767
    - model: Packages
      AP: 0.687
      mm_gdino_t_br_o365_goldg: 0.707
      mm_gdino_t_br_o365_goldg_grit: 0.687
      mm_gdino_t_br_o365_goldg_v3det: 0.71
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.706
    - model: PascalVOC
      AP: 0.563
      mm_gdino_t_br_o365_goldg: 0.565
      mm_gdino_t_br_o365_goldg_grit: 0.58
      mm_gdino_t_br_o365_goldg_v3det: 0.556
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.566
    - model: pistols
      AP: 0.726
      mm_gdino_t_br_o365_goldg: 0.585
      mm_gdino_t_br_o365_goldg_grit: 0.709
      mm_gdino_t_br_o365_goldg_v3det: 0.671
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.729
    - model: pothole
      AP: 0.215
      mm_gdino_t_br_o365_goldg: 0.136
      mm_gdino_t_br_o365_goldg_grit: 0.285
      mm_gdino_t_br_o365_goldg_v3det: 0.199
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.243
    - model: Raccoon
      AP: 0.549
      mm_gdino_t_br_o365_goldg: 0.469
      mm_gdino_t_br_o365_goldg_grit: 0.511
      mm_gdino_t_br_o365_goldg_v3det: 0.553
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.535
    - model: ShellfishOpenImages
      AP: 0.393
      mm_gdino_t_br_o365_goldg: 0.321
      mm_gdino_t_br_o365_goldg_grit: 0.437
      mm_gdino_t_br_o365_goldg_v3det: 0.519
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.488
    - model: thermalDogsAndPeople
      AP: 0.657
      mm_gdino_t_br_o365_goldg: 0.556
      mm_gdino_t_br_o365_goldg_grit: 0.603
      mm_gdino_t_br_o365_goldg_v3det: 0.493
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.542
    - model: VehiclesOpenImages
      AP: 0.613
      mm_gdino_t_br_o365_goldg: 0.566
      mm_gdino_t_br_o365_goldg_grit: 0.603
      mm_gdino_t_br_o365_goldg_v3det: 0.614
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.615
    - model: Average
      AP: 0.514
      mm_gdino_t_br_o365_goldg: 0.453
      mm_gdino_t_br_o365_goldg_grit: 0.511
      mm_gdino_t_br_o365_goldg_v3det: 0.516
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.533
    - model: AerialMaritimeDrone_large
      AP: 0.173
      mm_gdino_t_br_o365_goldg: 0.133
      mm_gdino_t_br_o365_goldg_grit: 0.155
      mm_gdino_t_br_o365_goldg_v3det: 0.177
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.151
    - model: AerialMaritimeDrone_tiled
      AP: 0.206
      mm_gdino_t_br_o365_goldg: 0.17
      mm_gdino_t_br_o365_goldg_grit: 0.225
      mm_gdino_t_br_o365_goldg_v3det: 0.184
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.206
    - model: AmericanSignLanguageLetters
      AP: 0.002
      mm_gdino_t_br_o365_goldg: 0.016
      mm_gdino_t_br_o365_goldg_grit: 0.02
      mm_gdino_t_br_o365_goldg_v3det: 0.011
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.007
    - model: Aquarium
      AP: 0.195
      mm_gdino_t_br_o365_goldg: 0.252
      mm_gdino_t_br_o365_goldg_grit: 0.261
      mm_gdino_t_br_o365_goldg_v3det: 0.266
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.283
    - model: BCCD
      AP: 0.161
      mm_gdino_t_br_o365_goldg: 0.069
      mm_gdino_t_br_o365_goldg_grit: 0.118
      mm_gdino_t_br_o365_goldg_v3det: 0.083
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.077
    - model: boggleBoards
      AP: 0.0
      mm_gdino_t_br_o365_goldg: 0.002
      mm_gdino_t_br_o365_goldg_grit: 0.001
      mm_gdino_t_br_o365_goldg_v3det: 0.001
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.002
    - model: brackishUnderwater
      AP: 0.021
      mm_gdino_t_br_o365_goldg: 0.033
      mm_gdino_t_br_o365_goldg_grit: 0.021
      mm_gdino_t_br_o365_goldg_v3det: 0.025
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.025
    - model: ChessPieces
      AP: 0.0
      mm_gdino_t_br_o365_goldg: 0.0
      mm_gdino_t_br_o365_goldg_grit: 0.0
      mm_gdino_t_br_o365_goldg_v3det: 0.0
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.0
    - model: CottontailRabbits
      AP: 0.806
      mm_gdino_t_br_o365_goldg: 0.771
      mm_gdino_t_br_o365_goldg_grit: 0.81
      mm_gdino_t_br_o365_goldg_v3det: 0.778
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.786
    - model: dice
      AP: 0.004
      mm_gdino_t_br_o365_goldg: 0.002
      mm_gdino_t_br_o365_goldg_grit: 0.005
      mm_gdino_t_br_o365_goldg_v3det: 0.001
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.001
    - model: DroneControl
      AP: 0.042
      mm_gdino_t_br_o365_goldg: 0.047
      mm_gdino_t_br_o365_goldg_grit: 0.097
      mm_gdino_t_br_o365_goldg_v3det: 0.088
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.074
    - model: EgoHands_generic
      AP: 0.608
      mm_gdino_t_br_o365_goldg: 0.527
      mm_gdino_t_br_o365_goldg_grit: 0.537
      mm_gdino_t_br_o365_goldg_v3det: 0.506
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.519
    - model: EgoHands_specific
      AP: 0.002
      mm_gdino_t_br_o365_goldg: 0.001
      mm_gdino_t_br_o365_goldg_grit: 0.005
      mm_gdino_t_br_o365_goldg_v3det: 0.007
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.003
    - model: HardHatWorkers
      AP: 0.046
      mm_gdino_t_br_o365_goldg: 0.048
      mm_gdino_t_br_o365_goldg_grit: 0.07
      mm_gdino_t_br_o365_goldg_v3det: 0.07
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.108
    - model: MaskWearing
      AP: 0.004
      mm_gdino_t_br_o365_goldg: 0.009
      mm_gdino_t_br_o365_goldg_grit: 0.004
      mm_gdino_t_br_o365_goldg_v3det: 0.011
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.009
    - model: MountainDewCommercial
      AP: 0.43
      mm_gdino_t_br_o365_goldg: 0.453
      mm_gdino_t_br_o365_goldg_grit: 0.465
      mm_gdino_t_br_o365_goldg_v3det: 0.194
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.43
    - model: NorthAmericaMushrooms
      AP: 0.471
      mm_gdino_t_br_o365_goldg: 0.331
      mm_gdino_t_br_o365_goldg_grit: 0.462
      mm_gdino_t_br_o365_goldg_v3det: 0.669
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.767
    - model: openPoetryVision
      AP: 0.0
      mm_gdino_t_br_o365_goldg: 0.001
      mm_gdino_t_br_o365_goldg_grit: 0.0
      mm_gdino_t_br_o365_goldg_v3det: 0.0
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.0
    - model: OxfordPets_by_breed
      AP: 0.003
      mm_gdino_t_br_o365_goldg: 0.002
      mm_gdino_t_br_o365_goldg_grit: 0.004
      mm_gdino_t_br_o365_goldg_v3det: 0.006
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.004
    - model: OxfordPets_by_species
      AP: 0.011
      mm_gdino_t_br_o365_goldg: 0.019
      mm_gdino_t_br_o365_goldg_grit: 0.016
      mm_gdino_t_br_o365_goldg_v3det: 0.02
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.015
    - model: PKLot
      AP: 0.001
      mm_gdino_t_br_o365_goldg: 0.004
      mm_gdino_t_br_o365_goldg_grit: 0.002
      mm_gdino_t_br_o365_goldg_v3det: 0.008
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.007
    - model: Packages
      AP: 0.695
      mm_gdino_t_br_o365_goldg: 0.707
      mm_gdino_t_br_o365_goldg_grit: 0.687
      mm_gdino_t_br_o365_goldg_v3det: 0.71
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.706
    - model: PascalVOC
      AP: 0.563
      mm_gdino_t_br_o365_goldg: 0.565
      mm_gdino_t_br_o365_goldg_grit: 0.58
      mm_gdino_t_br_o365_goldg_v3det: 0.566
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.566
    - model: pistols
      AP: 0.726
      mm_gdino_t_br_o365_goldg: 0.585
      mm_gdino_t_br_o365_goldg_grit: 0.709
      mm_gdino_t_br_o365_goldg_v3det: 0.671
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.729
    - model: plantdoc
      AP: 0.005
      mm_gdino_t_br_o365_goldg: 0.005
      mm_gdino_t_br_o365_goldg_grit: 0.007
      mm_gdino_t_br_o365_goldg_v3det: 0.008
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.011
    - model: pothole
      AP: 0.215
      mm_gdino_t_br_o365_goldg: 0.136
      mm_gdino_t_br_o365_goldg_grit: 0.219
      mm_gdino_t_br_o365_goldg_v3det: 0.077
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.168
    - model: Raccoons
      AP: 0.549
      mm_gdino_t_br_o365_goldg: 0.469
      mm_gdino_t_br_o365_goldg_grit: 0.511
      mm_gdino_t_br_o365_goldg_v3det: 0.553
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.535
    - model: selfdrivingCar
      AP: 0.089
      mm_gdino_t_br_o365_goldg: 0.091
      mm_gdino_t_br_o365_goldg_grit: 0.076
      mm_gdino_t_br_o365_goldg_v3det: 0.094
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.083
    - model: ShellfishOpenImages
      AP: 0.393
      mm_gdino_t_br_o365_goldg: 0.321
      mm_gdino_t_br_o365_goldg_grit: 0.437
      mm_gdino_t_br_o365_goldg_v3det: 0.519
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.488
    - model: ThermalCheetah
      AP: 0.087
      mm_gdino_t_br_o365_goldg: 0.063
      mm_gdino_t_br_o365_goldg_grit: 0.081
      mm_gdino_t_br_o365_goldg_v3det: 0.03
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.045
    - model: thermalDogsAndPeople
      AP: 0.657
      mm_gdino_t_br_o365_goldg: 0.556
      mm_gdino_t_br_o365_goldg_grit: 0.603
      mm_gdino_t_br_o365_goldg_v3det: 0.493
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.543
    - model: UnoCards
      AP: 0.006
      mm_gdino_t_br_o365_goldg: 0.012
      mm_gdino_t_br_o365_goldg_grit: 0.01
      mm_gdino_t_br_o365_goldg_v3det: 0.009
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.005
    - model: VehiclesOpenImages
      AP: 0.613
      mm_gdino_t_br_o365_goldg: 0.566
      mm_gdino_t_br_o365_goldg_grit: 0.603
      mm_gdino_t_br_o365_goldg_v3det: 0.614
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.615
    - model: WildfireSmoke
      AP: 0.134
      mm_gdino_t_br_o365_goldg: 0.106
      mm_gdino_t_br_o365_goldg_grit: 0.154
      mm_gdino_t_br_o365_goldg_v3det: 0.042
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.127
    - model: websiteScreenshots
      AP: 0.012
      mm_gdino_t_br_o365_goldg: 0.02
      mm_gdino_t_br_o365_goldg_grit: 0.016
      mm_gdino_t_br_o365_goldg_v3det: 0.016
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.016
    - model: Average
      AP: 0.227
      mm_gdino_t_br_o365_goldg: 0.202
      mm_gdino_t_br_o365_goldg_grit: 0.228
      mm_gdino_t_br_o365_goldg_v3det: 0.214
      mm_gdino_t_br_o365_goldg_grit_v3det: 0.284
    - model: RefCOCO val @1,5,10
      AP: 50.8
    - model: RefCOCO testA @1,5,10
      AP: 57.4
    - model: RefCOCO testB @1,5,10
      AP: 45.0
    - model: RefCOCO+ val @1,5,10
      AP: 51.6
    - model: RefCOCO+ testA @1,5,10
      AP: 57.3
    - model: RefCOCO+ testB @1,5,10
      AP: 46.4
    - model: RefCOCOg val @1,5,10
      AP: 60.4
    - model: RefCOCOg test @1,5,10
      AP: 59.7
    - model: gRefCOCO val Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.5
      AP: 39.3
    - model: gRefCOCO val Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.6
      AP: 40.5
    - model: gRefCOCO val Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.7
      AP: 41.3
    - model: gRefCOCO val Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.8
      AP: 41.5
    - model: gRefCOCO testA Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.5
      AP: 31.9
    - model: gRefCOCO testA Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.6
      AP: 29.3
    - model: gRefCOCO testA Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.7
      AP: 27.2
    - model: gRefCOCO testA Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.8
      AP: 25.1
    - model: gRefCOCO testB Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.5
      AP: 30.9
    - model: gRefCOCO testB Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.6
      AP: 30.0
    - model: gRefCOCO testB Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.7
      AP: 29.7
    - model: gRefCOCO testB Pr@(F1=1, IoU≥0.5),N-acc
      thresh_score: 0.8
      AP: 29.1
    - model: FULL/short/middle/long/very long
      AP: 17.2
    - model: FULL/short/middle/long/very long
      AP: 22.3
    - model: PRES/short/middle/long/very long
      AP: 17.8
    - model: PRES/short/middle/long/very long
      AP: 21.0
    - model: ABS/short/middle/long/very long
      AP: 15.4
    - model: ABS/short/middle/long/very long
      AP: 26.0
    - model: R-50
      lr_schedule: 1x
      box_AP: 48.1
    - model: R-50
      lr_schedule: 1x
      box_AP: 50.8
    - model: R-50
      lr_schedule: 1x
      box_AP: 48.2
    - model: R-50
      lr_schedule: 1X
      box_AP: 50.8
    - model: Swin-T
      lr_schedule: ''
      box_AP: 49.8
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 69.1
    - model: R-50
      lr_schedule: 1x
      box_AP: 52.4
    - model: R-50
      lr_schedule: 1x
      box_AP: 55.3
    - model: R-50
      lr_schedule: 1x
      box_AP: 55.7
    - model: R-50
      lr_schedule: 1X
      box_AP: 57.4
    - model: Swin-T
      lr_schedule: ''
      box_AP: 29.8
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 65.5
    - model: R-50
      lr_schedule: 50e
      box_AP: 43.5
    - model: R-50
      lr_schedule: 50e
      box_AP: 46.2
    - model: R-50
      lr_schedule: 50e
      box_AP: 46.4
    - model: R-50
      lr_schedule: 50e
      box_AP: 48.6
    - model: Swin-T
      lr_schedule: 50e
      box_AP: 47.5
    - model: R-50
      lr_schedule: 50e
      box_AP: 30.1
    - model: R-50
      lr_schedule: 50e
      box_AP: 31.8
    - model: R-50
      lr_schedule: 50e
      box_AP: 34.5
    - model: R-50
      lr_schedule: 50e
      box_AP: 34.8
    - model: Swin-T
      lr_schedule: ''
      box_AP: 34.2
    - model: Swin-T
      lr_schedule: 50e
      box_AP: 51.5
    - model: R-50
      lr_schedule: 50e
      box_AP: 17.0
    - model: R-50
      lr_schedule: 50e
      box_AP: 18.0
    - model: R-50
      lr_schedule: 50e
      box_AP: 12.0
    - model: R-50
      lr_schedule: 50e
      box_AP: 13.4
    - model: Swin-T
      lr_schedule: ''
      box_AP: 23.1
    - model: Swin-T
      lr_schedule: 50e
      box_AP: 38.9
    - model: R-50
      lr_schedule: 1x
      box_AP: 37.4
    - model: R-50
      lr_schedule: 1x
      box_AP: 40.3
    - model: R-50
      lr_schedule: 1x
      box_AP: 39.4
    - model: R-50
      lr_schedule: 1X
      box_AP: 42.4
    - model: R-50
      lr_schedule: 1X
      box_AP: 50.1
    - model: Swin-T
      lr_schedule: ''
      box_AP: 46.6
    - model: Swin-T
      lr_schedule: ''
      box_AP: 48.5
    - model: Swin-T
      lr_schedule: ''
      box_AP: 50.4
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 55.4
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 58.1
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 58.2
    - model: Swin-T
      lr_schedule: ''
      box_AP: 46.7
    - model: Swin-T
      lr_schedule: ''
      box_AP: 48.5
    - model: Swin-T
      lr_schedule: ''
      box_AP: 50.4
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 54.7
    - model: Swin-T
      lr_schedule: ''
      box_AP: 74.2
    - model: Swin-T
      lr_schedule: 1x
      box_AP: 75.3
    - model: Swin-T
      lr_schedule: ''
      AP: 19.6
    - model: Swin-T
      lr_schedule: ''
      AP: 20.1
    - model: Swin-T
      lr_schedule: ''
      AP: 31.9
    - model: Swin-T
      lr_schedule: 1x
      AP: 51.7
    - model: Swin-T
      lr_schedule: ''
      AP: 41.4
    - model: Swin-T
      lr_schedule: 1x
      AP: 57.1
  - repository: mmdetection
    file: configs/mask2former_vis/README.md
    config: mask2former_vis
    algorithm: Mask2Former for Video Instance Segmentation
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 8e
      mem_gb: 6.0
      AP: 41.3
    - model: R-101
      style: pytorch
      lr_schedule: 8e
      mem_gb: 7.5
      AP: 42.3
    - model: Swin-L
      style: pytorch
      lr_schedule: 8e
      mem_gb: 18.5
      AP: 52.3
  - repository: mmdetection
    file: configs/regnet/README.md
    config: regnet
    algorithm: RegNet
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.4
      fps: 12.0
      box_AP: 38.2
      mask_AP: 34.7
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.0
      box_AP: 40.3
      mask_AP: 36.6
    - model: RegNetX-4.0GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.5
      box_AP: 41.5
      mask_AP: 37.4
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.4
      fps: 10.3
      box_AP: 40.0
      mask_AP: 36.1
    - model: RegNetX-6.4GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.1
      box_AP: 41.0
      mask_AP: 37.1
    - model: X-101-32x4d-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.6
      fps: 9.4
      box_AP: 41.9
      mask_AP: 37.5
    - model: RegNetX-8.0GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.4
      box_AP: 41.7
      mask_AP: 37.5
    - model: RegNetX-12GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 7.4
      box_AP: 42.2
      mask_AP: 38.0
    - model: RegNetX-3.2GF-FPN-DCN-C3-C5
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.0
      box_AP: 40.3
      mask_AP: 36.6
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.0
      fps: 18.2
      box_AP: 37.4
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.5
      box_AP: 39.9
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 2x
      mem_gb: 4.5
      box_AP: 41.1
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.8
      fps: 16.6
      box_AP: 36.5
    - model: RegNetX-800MF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 2.5
      box_AP: 35.6
    - model: RegNetX-1.6GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.3
      box_AP: 37.3
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.2
      box_AP: 39.1
    - model: RegNetX-400MF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 2.3
      box_AP: 37.1
    - model: RegNetX-800MF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 2.8
      box_AP: 38.8
    - model: RegNetX-1.6GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 3.4
      box_AP: 40.5
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 4.4
      box_AP: 42.3
    - model: RegNetX-4GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 4.9
      box_AP: 42.8
    - model: RegNetX-400MF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 2.5
      box_AP: 37.6
      mask_AP: 34.4
    - model: RegNetX-800MF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 2.9
      box_AP: 39.5
      mask_AP: 36.1
    - model: RegNetX-1.6GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 3.6
      box_AP: 40.9
      mask_AP: 37.5
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.0
      box_AP: 43.1
      mask_AP: 38.7
    - model: RegNetX-4GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.1
      box_AP: 43.4
      mask_AP: 39.2
    - model: RegNetX-400MF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 4.3
      box_AP: 41.6
      mask_AP: 36.4
    - model: RegNetX-800MF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 4.8
      box_AP: 42.8
      mask_AP: 37.6
    - model: RegNetX-1.6GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 5.4
      box_AP: 44.5
      mask_AP: 39.0
    - model: RegNetX-3.2GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 6.4
      box_AP: 45.8
      mask_AP: 40.0
    - model: RegNetX-4GF-FPN
      style: pytorch
      lr_schedule: 3x
      mem_gb: 6.9
      box_AP: 45.8
      mask_AP: 40.0
  - repository: mmdetection
    file: configs/detectors/README.md
    config: detectors
    algorithm: DetectoRS
    models:
    - model: RFP
      lr_schedule: 1x
      mem_gb: 7.5
      box_AP: 44.8
    - model: SAC
      lr_schedule: 1x
      mem_gb: 5.6
      box_AP: 45.0
    - model: DetectoRS
      lr_schedule: 1x
      mem_gb: 9.9
      box_AP: 47.4
    - model: RFP
      lr_schedule: 1x
      mem_gb: 11.2
      box_AP: 46.6
      mask_AP: 40.9
    - model: SAC
      lr_schedule: 1x
      mem_gb: 9.3
      box_AP: 46.4
      mask_AP: 40.9
    - model: DetectoRS
      lr_schedule: 1x
      mem_gb: 13.6
      box_AP: 49.1
      mask_AP: 42.6
    - model: DetectoRS
      lr_schedule: 20e
      mem_gb: 19.6
      box_AP: 50.5
      mask_AP: 43.9
  - repository: mmdetection
    file: configs/cornernet/README.md
    config: cornernet
    algorithm: CornerNet
    models:
    - model: HourglassNet-104
      mem_gb: 13.9
      fps: 4.2
      box_AP: 41.2
    - model: HourglassNet-104
      mem_gb: 15.9
      fps: 4.2
      box_AP: 41.2
    - model: HourglassNet-104
      mem_gb: 9.5
      fps: 3.9
      box_AP: 40.4
  - repository: mmdetection
    file: configs/yolo/README.md
    config: yolo
    algorithm: YOLOv3
    models:
    - model: DarkNet-53
      scale: 320.0
      lr_schedule: 273e
      mem_gb: 2.7
      fps: 63.9
      box_AP: 27.9
    - model: DarkNet-53
      scale: 416.0
      lr_schedule: 273e
      mem_gb: 3.8
      fps: 61.2
      box_AP: 30.9
    - model: DarkNet-53
      scale: 608.0
      lr_schedule: 273e
      mem_gb: 7.4
      fps: 48.1
      box_AP: 33.7
    - model: DarkNet-53
      scale: 608.0
      lr_schedule: 273e
      mem_gb: 4.7
      fps: 48.1
      box_AP: 33.8
    - model: MobileNetV2
      scale: 416.0
      lr_schedule: 300e
      mem_gb: 5.3
      box_AP: 23.9
    - model: MobileNetV2
      scale: 320.0
      lr_schedule: 300e
      mem_gb: 3.2
      box_AP: 22.2
  - repository: mmdetection
    file: configs/free_anchor/README.md
    config: free_anchor
    algorithm: FreeAnchor
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.9
      fps: 18.4
      box_AP: 38.7
    - model: R-101
      style: pytorch
      lr_schedule: 1x
      mem_gb: 6.8
      fps: 14.9
      box_AP: 40.3
    - model: X-101-32x4d
      style: pytorch
      lr_schedule: 1x
      mem_gb: 8.1
      fps: 11.1
      box_AP: 41.9
  - repository: mmdetection
    file: configs/legacy_1.x/README.md
    config: legacy_1.x
    algorithm: Legacy Configs in MMDetection V1.x
    models:
    - model: Mask R-CNN R-50-FPN
      style: pytorch
      lr_schedule: 1x
      box_AP: 36.8
      mask_AP: 33.9
    - model: RetinaNet R-50-FPN
      style: caffe
      lr_schedule: 1x
      box_AP: 35.4
    - model: RetinaNet R-50-FPN
      style: pytorch
      lr_schedule: 1x
      box_AP: 35.2
    - model: Cascade Mask R-CNN R-50-FPN
      style: pytorch
      lr_schedule: 1x
      box_AP: 40.8
      mask_AP: 35.6
    - model: SSD300-VGG16
      style: caffe
      lr_schedule: 120e
      box_AP: 25.4
  - repository: mmdetection
    file: configs/detr/README.md
    config: detr
    algorithm: DETR
    models:
    - model: DETR
      lr_schedule: 150e
      mem_gb: 7.9
      box_AP: 39.9
  - repository: mmdetection
    file: configs/foveabox/README.md
    config: foveabox
    algorithm: FoveaBox
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.6
      fps: 24.1
      box_AP: 36.5
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      mem_gb: 5.6
      box_AP: 37.2
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      mem_gb: 8.1
      fps: 19.4
      box_AP: 37.9
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      mem_gb: 8.1
      fps: 18.3
      box_AP: 40.4
    - model: R-101
      style: pytorch
      lr_schedule: 1x
      mem_gb: 9.2
      fps: 17.4
      box_AP: 38.6
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      mem_gb: 11.7
      box_AP: 40.0
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      mem_gb: 11.7
      fps: 14.7
      box_AP: 40.0
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      mem_gb: 11.7
      fps: 14.7
      box_AP: 42.0
  - repository: mmdetection
    file: configs/condinst/README.md
    config: condinst
    algorithm: CondInst
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      box_AP: 39.8
      mask_AP: 36.0
  - repository: mmdetection
    file: configs/cityscapes/README.md
    config: cityscapes
    algorithm: Cityscapes
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.2
      box_AP: 40.3
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.3
      box_AP: 40.9
      mask_AP: 36.4
  - repository: mmdetection
    file: configs/paa/README.md
    config: paa
    algorithm: PAA
    models:
    - model: R-50-FPN
      lr_schedule: 12e
      mem_gb: 3.7
      box_AP: 40.4
    - model: R-50-FPN
      lr_schedule: 12e
      mem_gb: 3.7
      box_AP: 40.2
    - model: R-50-FPN
      lr_schedule: 18e
      mem_gb: 3.7
      box_AP: 41.4
    - model: R-50-FPN
      lr_schedule: 18e
      mem_gb: 3.7
      box_AP: 41.2
    - model: R-50-FPN
      lr_schedule: 24e
      mem_gb: 3.7
      box_AP: 41.6
    - model: R-50-FPN
      lr_schedule: 36e
      mem_gb: 3.7
      box_AP: 43.3
    - model: R-101-FPN
      lr_schedule: 12e
      mem_gb: 6.2
      box_AP: 42.6
    - model: R-101-FPN
      lr_schedule: 12e
      mem_gb: 6.2
      box_AP: 42.4
    - model: R-101-FPN
      lr_schedule: 24e
      mem_gb: 6.2
      box_AP: 43.5
    - model: R-101-FPN
      lr_schedule: 36e
      mem_gb: 6.2
      box_AP: 45.1
  - repository: mmdetection
    file: configs/seesaw_loss/README.md
    config: seesaw_loss
    algorithm: Seesaw Loss
    models:
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 25.6
      mask_AP: 25.0
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 25.6
      mask_AP: 25.4
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 27.4
      mask_AP: 26.7
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 27.2
      mask_AP: 27.3
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 27.6
      mask_AP: 26.4
    - model: R-50-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 27.6
      mask_AP: 26.8
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 28.9
      mask_AP: 27.6
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 28.9
      mask_AP: 28.2
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 33.1
      mask_AP: 29.2
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 33.0
      mask_AP: 30.0
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 30.0
      mask_AP: 29.3
    - model: R-101-FPN
      style: pytorch
      lr_schedule: 2x
      box_AP: 32.8
      mask_AP: 30.1
  - repository: mmdetection
    file: configs/tood/README.md
    config: tood
    algorithm: TOOD
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.1
      box_AP: 42.4
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 4.1
      box_AP: 42.4
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      mem_gb: 4.1
      box_AP: 44.5
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      mem_gb: 6.0
      box_AP: 46.1
    - model: R-101-dcnv2
      style: pytorch
      lr_schedule: 2x
      mem_gb: 6.2
      box_AP: 49.3
    - model: X-101-64x4d
      style: pytorch
      lr_schedule: 2x
      mem_gb: 10.2
      box_AP: 47.6
    - model: X-101-64x4d-dcnv2
      style: pytorch
      lr_schedule: 2x
  - repository: mmdetection
    file: configs/fpg/README.md
    config: fpg
    algorithm: FPG
    models:
    - model: Faster R-CNN
      lr_schedule: 50e
      mem_gb: 20.0
      box_AP: 42.3
    - model: Faster R-CNN
      lr_schedule: 50e
      mem_gb: 11.9
      box_AP: 41.2
    - model: Faster R-CNN
      lr_schedule: 50e
      mem_gb: 20.0
      box_AP: 38.9
    - model: Mask R-CNN
      lr_schedule: 50e
      mem_gb: 23.2
      box_AP: 43.0
      mask_AP: 38.1
    - model: Mask R-CNN
      lr_schedule: 50e
      mem_gb: 15.3
      box_AP: 41.7
      mask_AP: 37.1
    - model: Mask R-CNN
      lr_schedule: 50e
      mem_gb: 23.2
      box_AP: 49.6
      mask_AP: 35.6
    - model: RetinaNet
      lr_schedule: 50e
      mem_gb: 20.8
      box_AP: 40.5
    - model: RetinaNet
      lr_schedule: 50e
      mem_gb: 19.9
      box_AP: 39.9
  - repository: mmdetection
    file: configs/atss/README.md
    config: atss
    algorithm: ATSS
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      mem_gb: 3.7
      fps: 19.7
      box_AP: 39.4
    - model: R-101
      style: pytorch
      lr_schedule: 1x
      mem_gb: 5.6
      fps: 12.3
      box_AP: 41.5
  - repository: mmdetection
    file: configs/autoassign/README.md
    config: autoassign
    algorithm: AutoAssign
    models:
    - model: R-50
      style: caffe
      lr_schedule: 1x
      mem_gb: 4.08
      box_AP: 40.4
  - repository: mmdetection
    file: configs/simple_copy_paste/README.md
    config: simple_copy_paste
    algorithm: SimpleCopyPaste
    models:
    - model: R-50
      batch_size: 64.0
      box_AP: 43.3
      mask_AP: 39.0
    - model: R-50
      batch_size: 64.0
      box_AP: 43.8
      mask_AP: 39.2
    - model: R-50
      batch_size: 64.0
      box_AP: 43.5
      mask_AP: 39.1
    - model: R-50
      batch_size: 64.0
      box_AP: 45.1
      mask_AP: 40.3
  - repository: mmdetection
    file: configs/nas_fcos/README.md
    config: nas_fcos
    algorithm: NAS-FCOS
    models:
    - model: R-50
      style: caffe
      lr_schedule: 1x
      box_AP: 39.4
    - model: R-50
      style: caffe
      lr_schedule: 1x
      box_AP: 38.5
  - repository: mmdetection
    file: configs/timm_example/README.md
    config: timm_example
    algorithm: Timm Example
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
    - model: EfficientNet-B1
      style: '-'
      lr_schedule: 1x
  - repository: mmdetection
    file: configs/gfl/README.md
    config: gfl
    algorithm: GFL
    models:
    - model: R-50
      style: pytorch
      lr_schedule: 1x
      fps: 19.5
      box_AP: 40.2
    - model: R-50
      style: pytorch
      lr_schedule: 2x
      fps: 19.5
      box_AP: 42.9
    - model: R-101
      style: pytorch
      lr_schedule: 2x
      fps: 14.7
      box_AP: 44.7
    - model: R-101-dcnv2
      style: pytorch
      lr_schedule: 2x
      fps: 12.9
      box_AP: 47.1
    - model: X-101-32x4d
      style: pytorch
      lr_schedule: 2x
      fps: 12.1
      box_AP: 45.9
    - model: X-101-32x4d-dcnv2
      style: pytorch
      lr_schedule: 2x
      fps: 10.7
      box_AP: 48.1
  - repository: mmdetection
    file: configs/soft_teacher/README.md
    config: soft_teacher
    algorithm: SoftTeacher
    models:
    - model: SoftTeacher
      box_AP: 19.9
    - model: SoftTeacher
      box_AP: 24.9
    - model: SoftTeacher
      box_AP: 30.4
    - model: SoftTeacher
      box_AP: 33.8
  mmpose:
  - repository: mmpose
    file: configs/fashion_2d_keypoint/topdown_heatmap/README.md
    config: fashion_2d_keypoint
    algorithm: Top-down heatmap-based pose estimation
    models:
    - model: HRNet-w48-UDP-Upper
      PCK: 96.1
      AUC: 60.9
      EPE: 15.1
    - model: HRNet-w48-UDP-Lower
      PCK: 97.8
      AUC: 76.1
      EPE: 8.9
    - model: HRNet-w48-UDP-Full
      PCK: 98.3
      AUC: 67.3
      EPE: 11.7
    - model: ResNet-50-Upper
      PCK: 95.4
      AUC: 57.8
      EPE: 16.8
    - model: ResNet-50-Lower
      PCK: 96.5
      AUC: 74.4
      EPE: 10.5
    - model: ResNet-50-Full
      PCK: 97.7
      AUC: 66.4
      EPE: 12.7
    - model: ResNet-50-Short-Sleeved-Shirt
      PCK: 0.988
      AUC: 0.703
      EPE: 10.2
    - model: ResNet-50-Long-Sleeved-Shirt
      PCK: 0.973
      AUC: 0.587
      EPE: 16.6
    - model: ResNet-50-Short-Sleeved-Outwear
      PCK: 0.966
      AUC: 0.408
      EPE: 24.0
    - model: ResNet-50-Long-Sleeved-Outwear
      PCK: 0.987
      AUC: 0.517
      EPE: 18.1
    - model: ResNet-50-Vest
      PCK: 0.981
      AUC: 0.643
      EPE: 12.7
    - model: ResNet-50-Sling
      PCK: 0.94
      AUC: 0.557
      EPE: 21.6
    - model: ResNet-50-Shorts
      PCK: 0.975
      AUC: 0.682
      EPE: 12.4
    - model: ResNet-50-Trousers
      PCK: 0.973
      AUC: 0.625
      EPE: 14.8
    - model: ResNet-50-Skirt
      PCK: 0.952
      AUC: 0.653
      EPE: 16.6
    - model: ResNet-50-Short-Sleeved-Dress
      PCK: 0.98
      AUC: 0.603
      EPE: 15.6
    - model: ResNet-50-Long-Sleeved-Dress
      PCK: 0.976
      AUC: 0.518
      EPE: 20.1
    - model: ResNet-50-Vest-Dress
      PCK: 0.98
      AUC: 0.6
      EPE: 16.0
    - model: ResNet-50-Sling-Dress
      PCK: 0.967
      AUC: 0.544
      EPE: 19.5
  - repository: mmpose
    file: configs/face_2d_keypoint/topdown_regression/README.md
    config: face_2d_keypoint
    algorithm: Top-down regression-based pose estimation
    models:
    - model: ResNet-50
      NME: 4.88
    - model: ResNet-50+WingLoss
      NME: 4.67
    - model: ResNet-50+SoftWingLoss
      NME: 4.44
  - repository: mmpose
    file: configs/face_2d_keypoint/rtmpose/README.md
    config: face_2d_keypoint
    algorithm: RTMPose
    models:
    - model: RTMPose-m
      NME: 0.0466
    - model: RTMPose-m
      NME: 4.01
    - model: RTMPose-m
      NME: 1.29
  - repository: mmpose
    file: configs/face_2d_keypoint/topdown_heatmap/README.md
    config: face_2d_keypoint
    algorithm: Top-down heatmap-based pose estimation
    models:
    - model: HRNetv2-w18
      NME: 4.1
    - model: HRNetv2-w18+Dark
      NME: 1.19
    - model: HRNetv2-w18
      NME: 1.27
    - model: HRNetv2-w18+Dark
      NME: 0.0513
    - model: SCNet-50
      NME: 0.0567
    - model: HRNetv2-w18
      NME: 0.0569
    - model: ResNet-50
      NME: 0.0582
    - model: HourglassNet
      NME: 0.0587
    - model: MobileNet-v2
      NME: 0.0611
    - model: HRNetv2-w18
      NME: 3.48
    - model: HRNetv2-w18+Dark
      NME: 4.29
    - model: HRNetv2-w18+AWing
      NME: 4.28
    - model: HRNetv2-w18
      NME: 4.33
  - repository: mmpose
    file: configs/body_2d_keypoint/topdown_regression/README.md
    config: body_2d_keypoint
    algorithm: Top-down regression-based pose estimation
    models:
    - model: ResNet-152+RLE
      AP: 0.731
      ar: 0.805
    - model: ResNet-101+RLE
      AP: 0.722
      ar: 0.768
    - model: ResNet-50+RLE
      AP: 0.706
      ar: 0.768
    - model: MobileNet-v2+RLE
      AP: 0.593
      ar: 0.644
    - model: ResNet-152
      AP: 0.584
      ar: 0.688
    - model: ResNet-101
      AP: 0.562
      ar: 0.67
    - model: ResNet-50
      AP: 0.528
      ar: 0.639
    - model: ResNet-50+RLE
      PCK: 0.277
    - model: ResNet-152
      PCK: 0.208
    - model: ResNet-101
      PCK: 0.2
    - model: ResNet-50
      PCK: 0.18
  - repository: mmpose
    file: configs/body_2d_keypoint/rtmpose/README.md
    config: body_2d_keypoint
    algorithm: RTMPose
    models:
    - model: RTMPose-t
      AP: 0.682
      ar: 0.736
    - model: RTMPose-s
      AP: 0.716
      ar: 0.768
    - model: RTMPose-m
      AP: 0.746
      ar: 0.795
    - model: RTMPose-l
      AP: 0.758
      ar: 0.806
    - model: RTMPose-t-aic-coco
      AP: 0.685
      ar: 0.738
    - model: RTMPose-s-aic-coco
      AP: 0.722
      ar: 0.772
    - model: RTMPose-m-aic-coco
      AP: 0.758
      ar: 0.806
    - model: RTMPose-l-aic-coco
      AP: 0.765
      ar: 0.813
    - model: RTMPose-m-aic-coco
      AP: 0.77
      ar: 0.816
    - model: RTMPose-l-aic-coco
      AP: 0.773
      ar: 0.819
    - model: RTMPose-m
      PCK: 0.348
    - model: RTMPose-m
      AP: 0.706
      ar: 0.788
    - model: RTMPose-s
      AP: 0.311
      ar: 0.381
    - model: RTMPose-m
      AP: 0.355
      ar: 0.417
    - model: RTMPose-l
      AP: 0.378
      ar: 0.442
    - model: RTMPose-s
      AP: 0.698
      ar: 0.732
    - model: RTMPose-m
      AP: 0.728
      ar: 0.759
    - model: RTMPose-l
      AP: 0.753
      ar: 0.783
  - repository: mmpose
    file: configs/body_2d_keypoint/simcc/README.md
    config: body_2d_keypoint
    algorithm: Top-down SimCC-based pose estimation
    models:
    - model: ResNet-50+SimCC
      AP: 0.735
      ar: 0.79
    - model: ResNet-50+SimCC
      AP: 0.721
      ar: 0.781
    - model: S-ViPNAS-MobileNet-V3+SimCC
      AP: 0.695
      ar: 0.755
    - model: MobileNet-V2+SimCC(wo/deconv)
      AP: 0.62
      ar: 0.678
  - repository: mmpose
    file: configs/body_2d_keypoint/integral_regression/README.md
    config: body_2d_keypoint
    algorithm: Top-down integral-regression-based pose estimation
    models:
    - model: ResNet-50+Debias-IPR
      AP: 0.675
      ar: 0.765
    - model: ResNet-50+DSNT
      AP: 0.674
      ar: 0.764
    - model: ResNet-50+IPR
      AP: 0.633
      ar: 0.73
  - repository: mmpose
    file: configs/body_2d_keypoint/topdown_heatmap/README.md
    config: body_2d_keypoint
    algorithm: Top-down heatmap-based pose estimation
    models:
    - model: ViTPose-h
      AP: 0.79
      ar: 0.84
    - model: HRNet-w48+UDP
      AP: 0.768
      ar: 0.817
    - model: MSPN 4-stg
      AP: 0.765
      ar: 0.826
    - model: HRNet-w48+Dark
      AP: 0.764
      ar: 0.814
    - model: HRNet-w48
      AP: 0.756
      ar: 0.809
    - model: HRFormer-B
      AP: 0.754
      ar: 0.807
    - model: RSN-50-3x
      AP: 0.75
      ar: 0.814
    - model: CSPNeXt-l
      AP: 0.75
      ar: 0.8
    - model: HRNet-w32
      AP: 0.749
      ar: 0.804
    - model: Swin-L
      AP: 0.743
      ar: 0.798
    - model: ViTPose-s
      AP: 0.739
      ar: 0.792
    - model: HRFormer-S
      AP: 0.738
      ar: 0.793
    - model: Swin-B
      AP: 0.737
      ar: 0.794
    - model: SEResNet-101
      AP: 0.734
      ar: 0.79
    - model: SCNet-101
      AP: 0.733
      ar: 0.789
    - model: ResNet-101+Dark
      AP: 0.733
      ar: 0.786
    - model: CSPNeXt-m
      AP: 0.732
      ar: 0.785
    - model: ResNetV1d-101
      AP: 0.732
      ar: 0.785
    - model: SEResNet-50
      AP: 0.729
      ar: 0.784
    - model: SCNet-50
      AP: 0.728
      ar: 0.784
    - model: ResNet-101
      AP: 0.726
      ar: 0.783
    - model: ResNeXt-101
      AP: 0.726
      ar: 0.781
    - model: HourglassNet
      AP: 0.726
      ar: 0.78
    - model: ResNeSt-101
      AP: 0.725
      ar: 0.781
    - model: RSN-50
      AP: 0.724
      ar: 0.79
    - model: Swin-T
      AP: 0.724
      ar: 0.782
    - model: MSPN 1-stg
      AP: 0.723
      ar: 0.788
    - model: ResNetV1d-50
      AP: 0.722
      ar: 0.777
    - model: ResNeSt-50
      AP: 0.72
      ar: 0.775
    - model: ResNet-50
      AP: 0.718
      ar: 0.774
    - model: ResNeXt-50
      AP: 0.715
      ar: 0.771
    - model: PVT-S
      AP: 0.714
      ar: 0.773
    - model: CSPNeXt-s
      AP: 0.697
      ar: 0.753
    - model: LiteHRNet-30
      AP: 0.676
      ar: 0.736
    - model: CSPNeXt-tiny
      AP: 0.665
      ar: 0.723
    - model: MobileNet-v2
      AP: 0.648
      ar: 0.709
    - model: LiteHRNet-18
      AP: 0.642
      ar: 0.705
    - model: CPM
      AP: 0.627
      ar: 0.689
    - model: ShuffleNet-v2
      AP: 0.602
      ar: 0.668
    - model: ShuffleNet-v1
      AP: 0.587
      ar: 0.654
    - model: AlexNet
      AP: 0.448
      ar: 0.521
    - model: HRNet-w48+Dark
      PCK: 0.36
    - model: HRNet-w48
      PCK: 0.303
    - model: HRNet-w48
      PCK: 0.337
    - model: HRNet-w32
      PCK: 0.334
    - model: HourglassNet
      PCK: 0.317
    - model: ResNet-152
      PCK: 0.303
    - model: ResNetV1d-152
      PCK: 0.3
    - model: SCNet-50
      PCK: 0.29
    - model: ResNeXt-152
      PCK: 0.294
    - model: SEResNet-50
      PCK: 0.292
    - model: ResNet-50
      PCK: 0.286
    - model: ResNetV1d-50
      PCK: 0.29
    - model: CPM
      PCK: 0.285
    - model: LiteHRNet-30
      PCK: 0.271
    - model: LiteHRNet-18
      PCK: 0.26
    - model: MobileNet-v2
      PCK: 0.234
    - model: ShuffleNet-v2
      PCK: 0.205
    - model: ShuffleNet-v1
      PCK: 0.195
    - model: HRNet-w32
      AP: 0.675
      ar: 0.816
    - model: CSPNeXt-m
      AP: 0.662
      ar: 0.755
    - model: ResNet-101
      AP: 0.647
      ar: 0.8
    - model: HRNet-w32
      AP: 0.637
      ar: 0.785
    - model: HRNet-w32
      AP: 0.323
      ar: 0.366
    - model: ResNet-101
      AP: 0.294
      ar: 0.337
    - model: ResNet-50
      PCK: 80.1
    - model: CPM
      PCK: 65.7
    - model: HRNet-w48
      AP: 84.6
    - model: HRNet-w32
      AP: 83.4
    - model: ResNet-50
      AP: 81.2
    - model: ViTPose-s
      AP: 0.381
      ar: 0.448
    - model: ViTPose-b
      AP: 0.41
      ar: 0.475
    - model: ViTPose-s
      AP: 0.738
      ar: 0.768
    - model: ViTPose-b
      AP: 0.759
      ar: 0.79
  - repository: mmpose
    file: configs/hand_3d_keypoint/internet/README.md
    config: hand_3d_keypoint
    algorithm: 'InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image'
    models:
    - model: InterNet_resnet_50
      mpjpe_single: 9.47
      mpjpe_interacting: 13.4
      mpjpe_all: 11.59
      mrrpe: 29.28
      AP: 0.99
    - model: InterNet_resnet_50
      mpjpe_single: 11.22
      mpjpe_interacting: 15.23
      mpjpe_all: 13.16
      mrrpe: 31.73
      AP: 0.98
  - repository: mmpose
    file: configs/wholebody_2d_keypoint/rtmpose/README.md
    config: wholebody_2d_keypoint
    algorithm: RTMPose
    models:
    - model: RTMPose-m
      AP: 0.582
      whole_ar: 0.674
    - model: RTMPose-l
      AP: 0.611
      whole_ar: 0.7
    - model: RTMPose-l
      AP: 0.648
      whole_ar: 0.73
  - repository: mmpose
    file: configs/wholebody_2d_keypoint/dwpose/README.md
    config: wholebody_2d_keypoint
    algorithm: DWPose
    models:
    - AP: 48.5
      whole_ar: 58.4
      flops_sup_br_g: 0.5
    - AP: 53.8
      whole_ar: 63.2
      flops_sup_br_g: 0.9
    - AP: 60.6
      whole_ar: 69.5
      flops_sup_br_g: 2.22
      ort_latency_sup_br_ms_sup_br_i7_11700: 13.5
      trt_fp16_latency_sup_br_ms_sup_br_gtx_1660ti: 4.0
    - AP: 63.1
      whole_ar: 71.7
      flops_sup_br_g: 4.52
      ort_latency_sup_br_ms_sup_br_i7_11700: 23.41
      trt_fp16_latency_sup_br_ms_sup_br_gtx_1660ti: 5.67
    - AP: 66.5
      whole_ar: 74.3
      flops_sup_br_g: 10.07
      ort_latency_sup_br_ms_sup_br_i7_11700: 44.58
      trt_fp16_latency_sup_br_ms_sup_br_gtx_1660ti: 7.68
  - repository: mmpose
    file: configs/wholebody_2d_keypoint/topdown_heatmap/README.md
    config: wholebody_2d_keypoint
    algorithm: Top-down heatmap-based pose estimation
    models:
    - model: HRNet-w48+Dark+
      AP: 0.661
      whole_ar: 0.743
    - model: HRNet-w32+Dark
      AP: 0.582
      whole_ar: 0.671
    - model: HRNet-w48
      AP: 0.579
      whole_ar: 0.681
    - model: CSPNeXt-m
      AP: 0.567
      whole_ar: 0.641
    - model: HRNet-w32
      AP: 0.549
      whole_ar: 0.646
    - model: ResNet-152
      AP: 0.548
      whole_ar: 0.661
    - model: HRNet-w32
      AP: 0.536
      whole_ar: 0.636
    - model: ResNet-101
      AP: 0.531
      whole_ar: 0.645
    - model: S-ViPNAS-Res50+Dark
      AP: 0.528
      whole_ar: 0.632
    - model: ResNet-50
      AP: 0.521
      whole_ar: 0.633
    - model: S-ViPNAS-Res50
      AP: 0.495
      whole_ar: 0.607
    - model: HRNet-w32
      AP: 0.69
      whole_ar: 0.729
  - repository: mmpose
    file: configs/animal_2d_keypoint/rtmpose/README.md
    config: animal_2d_keypoint
    algorithm: RTMPose
    models:
    - model: RTMPose-m
      AP: 0.722
  - repository: mmpose
    file: configs/animal_2d_keypoint/topdown_heatmap/README.md
    config: animal_2d_keypoint
    algorithm: Top-down heatmap-based pose estimation
    models:
    - model: HRNet-w32
      AP: 0.74
      ar: 0.78
    - model: HRNet-w48
      AP: 0.738
      ar: 0.778
    - model: ResNet-152
      AP: 0.704
      ar: 0.748
    - model: ResNet-101
      AP: 0.696
      ar: 0.736
    - model: ResNet-50
      AP: 0.691
      ar: 0.736
    - model: HRNet-w48
      AP: 0.728
    - model: HRNet-w32
      AP: 0.722
    - model: ResNet-101
      AP: 0.681
    - model: ResNet-50
      AP: 0.68
    - model: CSPNeXt-m
      AP: 0.703
    - model: ResNet-152
      AUC: 0.925
      EPE: 1.49
    - model: ResNet-101
      AUC: 0.907
      EPE: 2.03
    - model: ResNet-50
      AUC: 0.9
      EPE: 2.27
    - model: ResNet-152
      AUC: 0.921
      EPE: 1.67
    - model: ResNet-101
      AUC: 0.915
      EPE: 1.83
    - model: ResNet-50
      AUC: 0.914
      EPE: 1.87
    - model: HRNet-w32
      PCK: 0.6323
    - model: HRNet-w32
      PCK: 0.3741
    - model: HRNet-w32
      PCK: 0.571
    - model: HRNet-w32
      PCK: 0.5358
    - model: HRNet-w32
      PCK: 0.51
    - model: HRNet-w32
      PCK: 0.7671
    - model: HRNet-w32
      PCK: 0.6406
  - repository: mmpose
    file: configs/hand_2d_keypoint/topdown_regression/README.md
    config: hand_2d_keypoint
    algorithm: Top-down regression-based pose estimation
    models:
    - model: ResNet-50
      PCK: 0.99
      AUC: 0.485
      EPE: 34.21
    - model: ResNet-50
      PCK: 0.988
      AUC: 0.865
      EPE: 3.32
  - repository: mmpose
    file: configs/hand_2d_keypoint/rtmpose/README.md
    config: hand_2d_keypoint
    algorithm: RTMPose
    models:
    - model: RTMPose-m
      PCK: 0.815
      AUC: 0.837
      EPE: 4.51
  - repository: mmpose
    file: configs/hand_2d_keypoint/topdown_heatmap/README.md
    config: hand_2d_keypoint
    algorithm: Top-down heatmap-based pose estimation
    models:
    - model: HRNetv2-w18+Dark
      PCK: 0.814
      AUC: 0.84
      EPE: 4.37
    - model: HRNetv2-w18
      PCK: 0.813
      AUC: 0.84
      EPE: 4.39
    - model: HourglassNet
      PCK: 0.804
      AUC: 0.835
      EPE: 4.54
    - model: SCNet-50
      PCK: 0.803
      AUC: 0.834
      EPE: 4.55
    - model: ResNet-50
      PCK: 0.8
      AUC: 0.833
      EPE: 4.64
    - model: LiteHRNet-18
      PCK: 0.795
      AUC: 0.83
      EPE: 4.77
    - model: MobileNet-v2
      PCK: 0.795
      AUC: 0.829
      EPE: 4.77
    - model: ResNet-50
      PCK: 0.999
      AUC: 0.868
      EPE: 3.27
    - model: HRNetv2-w18+Dark
      PCK: 0.99
      AUC: 0.572
      EPE: 23.96
    - model: HRNetv2-w18+UDP
      PCK: 0.99
      AUC: 0.571
      EPE: 23.88
    - model: HRNetv2-w18
      PCK: 0.99
      AUC: 0.567
      EPE: 24.26
    - model: ResNet-50
      PCK: 0.989
      AUC: 0.555
      EPE: 25.16
    - model: MobileNet-v2
      PCK: 0.986
      AUC: 0.537
      EPE: 28.56
    - model: HRNetv2-w18+Dark
      PCK: 0.992
      AUC: 0.903
      EPE: 2.18
    - model: HRNetv2-w18+UDP
      PCK: 0.992
      AUC: 0.902
      EPE: 2.19
    - model: HRNetv2-w18
      PCK: 0.992
      AUC: 0.902
      EPE: 2.21
    - model: ResNet-50
      PCK: 0.991
      AUC: 0.898
      EPE: 2.32
    - model: MobileNet-v2
      PCK: 0.985
      AUC: 0.883
      EPE: 2.79
