# Configuration for Papers with Code Archive Scraper

# Base URL for the archived site
base_url: "https://web.archive.org/web/20240101123127/https://paperswithcode.com"

# Request settings
delay: 1.5  # Delay between requests in seconds (be respectful to archive.org)
timeout: 30  # Request timeout in seconds
max_retries: 3  # Maximum number of retries for failed requests
backoff_factor: 0.5  # Exponential backoff factor for retries

# User agent to identify the scraper
user_agent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"

# Data storage settings
data_dir: "scraped_data"  # Directory to store scraped data

# Scraping limits (for testing and rate limiting)
max_papers_per_task: 50  # Maximum papers to scrape per task
max_tasks: 10  # Maximum number of tasks to scrape
max_total_papers: 500  # Maximum total papers to scrape

# Categories to prioritize for scraping
priority_tasks:
  - "object-detection"
  - "semantic-segmentation"
  - "image-classification"
  - "machine-translation"
  - "question-answering"
  - "representation-learning"
  - "language-modelling"
  - "named-entity-recognition"
  - "pose-estimation"
  - "depth-estimation"

# URL patterns to identify different page types
url_patterns:
  paper: "/paper/"
  task: "/task/"
  dataset: "/dataset/"
  method: "/method/"
  author: "/author/"
  
# Archive.org specific settings
archive_date: "20240101123127"  # Specific archive snapshot to use

# Logging settings
log_level: "INFO"
log_file: "scraper.log"

# Checkpoint settings
checkpoint_interval: 50  # Save progress every N pages
resume_from_checkpoint: true  # Resume from last checkpoint if available